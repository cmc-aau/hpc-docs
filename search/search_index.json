{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the documentation website for the BioCloud HPC at the section of biotechnology at Aalborg University. Here you will find everything you need to use the cluster for large scale bioinformatic workflows and other things that require more compute resources and storage than normal computers have. The current setup consists of data center servers with dual socket AMD Epyc CPU's of various sizes (from 2022) with a total of 1848 vCPUs and 12 TB RAM, all connected to a 2.3 PB+ storage cluster (Ceph).</p> <p>The cluster is a shared resource for more than a hundred users, so to effectively manage the compute resources and allow many simultaneous users to run jobs concurrently in a completely isolated manner the cluster is set up with the SLURM workload manager.</p> <p>To gain access you must first ask an administrator to allow you to login using your AAU email, and then you will be able to use the cluster resources through either the web portal, or by submitting SLURM batch jobs through a remote shell on a login node.</p>"},{"location":"#quarterly-maintenance","title":"Quarterly maintenance","text":"<p>There are pre-scheduled quarterly maintenance days throughout the year on tuesdays in weeks 7, 20, 38, 49, where security upgrades will be performed on all servers, which will be rebooted any time during the day. There will be made a maintenance reservation in SLURM that ensures no jobs will be able to run on these days (and you will therefore see the reason code <code>(ReqNodeNotAvail, Reserved for maintenance)</code> if the time limit extends into this reservation), but jobs can still be submitted to the queue. Always stay tuned on the Teams channel for news and announcements.</p> 2024 2025 2026 2027 2028 Q1 (week 7) 19-03-2024 11-02-2025 10-02-2026 09-02-2027 08-02-2028 Q2 (week 20) 14-05-2024 13-05-2025 12-05-2026 11-05-2027 09-05-2028 Q3 (week 38) 17-09-2024 16-09-2025 15-09-2026 14-09-2027 12-09-2028 Q4 (week 49) 03-12-2024 02-12-2025 01-12-2026 30-11-2027 28-11-2028"},{"location":"storage/","title":"Storage","text":"<p>All nodes are connected to a Ceph network storage cluster through a few different mount points for different purposes, listed below. Data is stored with 3x replication to protect against data loss or corruption due to hardware and disk failures. The data is NOT backed up anywhere, however, which means it's not protected against human errors. If you delete data, it's gone and cannot be restored! Some mount points are therefore mounted read-only, but your own data is your responsibility alone. So be careful, also when sharing data with other users when doing shared projects etc.</p> <p>Data on the Ceph network storage cluster is lightly compressed at rest, so manually compressing files is not needed, however it can be beneficial in many ways to reduce the number of files.</p>"},{"location":"storage/#network-storage-mount-points","title":"Network storage mount points","text":"Mount point Permissions Contents <code>/home</code> Read/write on your own home folder only All data for individual projects and generally most things should go here <code>/projects</code> Read/write All data for shared projects should go here <code>/databases</code> Read-only Various databases required for bioinformatic tools <code>/raw_data</code> Read-only Raw data (mostly from DNA sequencing) that should never be touched nor deleted <code>/incoming</code> Read/write Incoming raw data that should be moved and organized in <code>/raw_data</code> as soon as possible <code>/nanopore</code> Read/write Incoming raw data from nanopore sequencing that should be moved and organized in <code>/raw_data</code> as soon as possible. This mount will be removed in the near future, please use <code>/incoming</code> instead. Max file size <p>There is a max file size of 4TB on the Ceph network cluster, which cannot be increased, details here.</p>"},{"location":"storage/#local-scratch-space","title":"Local scratch space","text":"<p>For jobs requiring heavy I/O where large amounts of data or thousands of temporary files need to be written, it's important to avoid using the network storage (if possible) and instead write to a local harddrive instead, which both avoids overloading the network and the storage cluster itself, but is also much faster in some cases. When deciding whether you need local scratch space, both the size of the data as well as the number of files are important, however the ladder is by far the most important because it has the biggest impact on the performance of the storage cluster overall, more info below.</p> <p>Local scratch space is always mounted at the same moint point on all machines, which is under <code>/scratch</code>. It's not available on all compute nodes (refer to hardware partitions), however, so if you need it you must ensure that you submit jobs to specific compute nodes using the <code>--nodelist</code> option to the <code>srun</code>, <code>sbatch</code>, and <code>salloc</code> commands. </p> <p>All data owned by your user anywhere under <code>/scratch</code> on a particular compute node is automatically deleted after the last SLURM job run by your user has finished. Therefore ensure to move files elsewhere as the last step of the job if needed, otherwise it will be lost. When using scratch space please create a separate folder there to work in, preferably just named by your username, so that data from other users' jobs that may also use the <code>/scratch</code> folder are not overwritten or otherwise impacted by your jobs.</p>"},{"location":"storage/#temporary-space","title":"Temporary space","text":"<p>Each job will have a separate and entirely private mount point for temporary data (default location is usually <code>/tmp</code> or <code>/var/tmp</code>), which will be automatically deleted after each job. On compute nodes with extra local scratch space it will automatically be mounted somewhere in there instead, so that there is a lot more space available for temporary files on those nodes (refer to hardware partitions).</p> <p>If you would need more space for temporary data on compute nodes that have no extra local scratch space, or you need even more temporary space than there's available on local scratch space, it's possible to place it on the Ceph network storage as well. However, if you choose to do so, please see the best practices below. It can simply be done by for example setting the environment variable <code>TMPDIR</code> early in the batch script by adding a line, fx <code>export TMPDIR=${HOME}/tmp</code>. Ensure no conflicts can occur within the folder(s) if you run multiple jobs on multiple different compute nodes at once.</p>"},{"location":"storage/#storage-policy","title":"Storage policy","text":"<p>Your data - your responsibility!</p> <p>The Ceph network storage is NOT a long-term storage or backup solution. EVERYTHING IS TEMPORARY, SO CLEAN UP AFTER YOURSELF IMMEDIATELY AFTER EACH JOB!! You will likely forget about it very quickly after you've done your work, so make it a habit to cleanup shortly after every job. Once studies are published, delete all the data and ensure that for example raw DNA sequencing data is uploaded to public archives and that the corresponding code to produce results in the studies is available somewhere, preferably in a GitHub repository.</p> <p>Furthermore, if you are no longer employed or studying at AAU, your home folder will be deleted without notice, and you have the sole responsibility to hand on any data that should be saved for longer by passing it on to other users, for example your PI, before you leave.</p>"},{"location":"storage/#moving-large-amounts-of-data-around","title":"Moving large amounts of data around","text":"<p>If you need to move large amounts of data (or numerous files at once regardless of size) it will happen in an instant regardless of the size if you make sure you are doing it on the same filesystem/mount, for example from <code>/projects/xxx</code> to <code>/projects/yyy</code>. However, if you need to move things across mount points, for example from <code>/home/xxx</code> to <code>/projects/yyy</code> please ask an administrator to do it for you, since moving between mount points will happen over the network and cause unneccessary traffic and load on the storage cluster, let alone be very slow. An administrator can move anything instantly anywhere on the storage cluster, but if you need to transfer external files there is no way around it, it will be limited by the network speed.</p>"},{"location":"storage/#best-practices-with-ceph-storage","title":"Best practices with Ceph storage","text":""},{"location":"storage/#avoid-numerous-small-files-at-all-costs","title":"Avoid numerous small files at all costs","text":"<p>Whereever possible always try to write fewer but larger files instead of many small ones. The Ceph storage cluster uses a file metadata cache server that holds an entry for every single open file, so the cache memory usage is directly proportional to the number of open files (meaning being accessed in any way). If there are too many open files (across ALL connected clients/nodes at once) the metadata server will ask clients to release some pressure on the cache and if they fail to do so in time they will simply be evicted without notice (meaning the Ceph cluster will force an unmount). This will happen on any node that tips the iceberg and is thus not a result of the exact storage actions of individual clients but rather that of all nodes connected to the Ceph cluster across the whole university at any one time. See Ceph best usage practices for additional details.</p>"},{"location":"storage/#avoid-using-ls-l","title":"Avoid using <code>ls -l</code>","text":"<p>When listing directories, it's common to use <code>ls -l</code> to list things vertically, however this will also request various other information like permissions, file size, owner, group, access time etc. This will burden the metadata servers, especially if used in loops in scripts on many files, so if you don't need all this extra information and just want to list the contents vertically instead of horizontally, just use <code>ls -1</code> instead and make that a habit. Likewise, don't use <code>stat</code> on many files if not neccessary.</p>"},{"location":"storage/#obtaining-the-total-size-of-folders","title":"Obtaining the total size of folders","text":"<p>To obtain the total disk space used of all files inside a folder it's common to use the <code>du -sh /some/folder</code> command. Doing this at a large folder is quite similar to performing a DDoS attack on the Ceph storage cluster, so please never use <code>du</code> on folders, only on individual files. It will likely never finish anyways if the folder contains many files. The best way to obtain the size of a folder is to instead obtain the information in the form of storage quota attributes directly from the Ceph metadata servers using the custom <code>storagequota</code> command as demonstrated below, which is both instant and will not cause any stress on the cluster:</p> <pre><code># home folder\n$ storagequota\nStorage quota used for '/home/bio.aau.dk/abc': 3.46TB / 9.09TB (38.06%)\n\n# specific folder\n$ storagequota /projects\nStorage quota used for '/projects': 397.54TB\n\n# multiple folders, sorted by size\nstoragequota /projects/* | sort -rt ':' -k2 -n | head -n 5\nStorage quota used for '/projects/microflora_danica':       208.40TB\nStorage quota used for '/projects/MiDAS':       125.75TB\nStorage quota used for '/projects/NNF_AD_2023':     30.66TB\nStorage quota used for '/projects/glomicave':       8.84TB\nStorage quota used for '/projects/dark_science':    7.19TB\n</code></pre> <p>The <code>storagequota</code> command is simply a convenient wrapper script around the <code>getfattr</code> command, which retrieves attributes of files and folders directly from the Ceph MDS servers. It only retrieves the size of the folder, but there are also a few other attributes that may be of interest, for example the number of files, see examples below.</p> <pre><code>$ getfattr -n ceph.dir.rfiles /projects \ngetfattr: Removing leading '/' from absolute path names\n# file: projects\nceph.dir.rfiles=\"219203126\"\n</code></pre> Additional Ceph attributes Attribute Explaination ceph.dir.entries ceph.dir.files Number of files in folder (non-recursive) ceph.dir.subdirs Number of subdirs (non-recursive) ceph.dir.rentries ceph.dir.rfiles Number of files in folder (recursive) ceph.dir.rsubdirs Number of folders in folder (recursive) ceph.dir.rbytes Size of folder (recursive) ceph.dir.rctime Recently changed time (non-recursive) <p>There are no descriptions on these in the Ceph documentation or anywhere else, I've simply found them in the Ceph source code! This is all there is.</p>"},{"location":"storage/#shared-folders","title":"Shared folders","text":"<p>If you need to give other users write access to a file/folder that you own, you need to set the group ownership of the folder to the <code>bio_server_users@bio.aau.dk</code> group and set the setGID bit on folders (to ensure child files/folders will inherit the ownership of a parent folder), see the example below. This will give everyone with access to the BioCloud servers full control of the files. If you only want a specific group of people to have write access, there is only one way to do that, which is to contact the university IT services to create an email address group for the specific users, and then follow the same steps below, but instead use the new email of that group.</p> <p>Never use <code>chmod 777</code>. It's a major security weakness that can allow anyone (even users outside of the university authentication system, human or not) to do anything they want with a file/folder and potentially place hidden payload there. Folders will be scanned regularly for insecure permissions and corrected without notice. Only <code>owner</code> and <code>group</code> should have the execute bit set, never <code>other</code>. See permissions in Linux to learn about file and folder permissions on Linux systems.</p> <pre><code>folder=\"some_folder/\"\n\n# create the folder if it doesn't exist already\nmkdir -p \"$folder\"\n\n# set group ownership\nchown -R :bio_server_users@bio.aau.dk \"$folder\"\n\n# If there are already files with weak permissions, correct them with:\nchmod -R o-x \"$folder\"\n\n# Ensure group can edit files/folders:\nfind \"$folder\" -type f -exec chmod 664 {} \\;\nfind \"$folder\" -type d -exec chmod 775 {} \\;\n\n# set the setGID sticky bit to ensure new files and folders inherit group ownership\nchmod 2775 \"$folder\"\n</code></pre>"},{"location":"access/ssh/","title":"Shell access through SSH","text":"<p>SSH (Secure Shell) is a widely used protocol for securely gaining access to a shell (or terminal) on remote Linux machines and is the primary way to access the BioCloud servers. This page provides instructions on how to access the BioCloud through SSH using a few different SSH clients and platforms. There are many other SSH clients available, it is entirely up to you which you prefer, but regardless of the client everything will run over the same SSH protocol (port 22) in the exact same way. You authenticate using your AAU email and password (possibly also a second factor) and you must be on the AAU network unless you are connected to the AAU network from the outside using either VPN or by using the AAU SSH gateway, both are described later under external access.</p> <p>If you need to run GUI apps like CLC, Arb, RStudio, VS Code, etc, you can instead use the interactive web portal described on the next page. For everything else, you need to learn how to submit SLURM jobs through one of the login-nodes <code>bio-ospikachu01.srv.aau.dk</code>, <code>bio-ospikachu02.srv.aau.dk</code>, or <code>bio-ospikachu03.srv.aau.dk</code>. After successfully logging in please consult the SLURM guide. You can then start developing code, write scripts, install software and browse around etc, but you are NOT ALLOWED to run anything on a SLURM login node, everything must be submitted through SLURM jobs.</p>"},{"location":"access/ssh/#code-editors-ides","title":"Code editors (IDEs)","text":"<p>It's rarely enough with just a terminal because you more often than not need to edit some scripts in order to do anything, which is not very convenient to do in a terminal, so below are some instructions on how to connect using a few popular code editors, or IDEs (integrated development environments), with built-in SSH support, but also just a terminal.</p>"},{"location":"access/ssh/#visual-studio-code","title":"Visual Studio Code","text":"<p>Visual Studio Code (VS Code) is a popular free cross-platform code editor with a myriad of extensions available for anything and everything including syntax highlighting for any programming language, integrated git support, GitHub copilot for AI autocompletion of code, and the list goes on. If you want one editor for everything, there isn't currently anything better out there. If you need an interactive VS Code session and actually run things from there, please follow this guide to connect VS Code directly to a SLURM job instead. Alternatively, you can also start a Code Server (an open source alternative to VS Code) in a SLURM job from the interactive web portal described on the next page.</p>"},{"location":"access/ssh/#installation-windows-macos-or-linux","title":"Installation (Windows, macOS, or Linux)","text":"<p>Download and install using the instructions on the official website.</p>"},{"location":"access/ssh/#connecting-to-a-login-node","title":"Connecting to a login-node","text":"<ol> <li>Open VS Code and install the \"Remote - SSH\" extension from the Extensions sidebar menu.</li> <li>Click on the \"Remote Explorer\" icon after the extension has been installed.</li> <li>Add a host by either:<ul> <li>clicking on the \"+\" icon and enter your AAU email followed by <code>@</code> and then the server's hostname, for example: <code>abc@bio.aau.dk@bio-ospikachu01.srv.aau.dk</code>, or</li> <li>add all servers at once using the SSH config template provided below by clicking the gear icon \"Open SSH Config File\" and paste its contents (optional). </li> </ul> </li> <li>Connect and log in with your SSH password.</li> <li>Once connected open a project or workspace folder (or create one while doing so) by clicking File -&gt; Open Folder (CTRL+k CTRL+o, CMD instead of CTRL if on macOS) to start your work</li> </ol>"},{"location":"access/ssh/#mobaxterm","title":"MobaXterm","text":"<p>A more old-school and Windows-only client is MobaXterm. It's not as full-featured as VS Code, but is more lightweight.</p>"},{"location":"access/ssh/#installation-windows-only","title":"Installation (Windows only)","text":"<p>Download and install MobaXterm Home Edition from the official website.</p>"},{"location":"access/ssh/#connecting-to-a-server","title":"Connecting to a server","text":"<ol> <li>Open MobaXterm and click on the \"Start local terminal\" button.</li> <li>In the terminal, use the <code>ssh</code> command to connect to a server by entering AAU email followed by <code>@</code> and then the server's hostname, for example: <code>ssh abc@bio.aau.dk@bio-ospikachu01.srv.aau.dk</code>.</li> </ol> <p>If you add the servers to your SSH config file the server hostnames should auto-complete.</p>"},{"location":"access/ssh/#just-a-terminal","title":"Just a terminal","text":"<p>For a simple terminal you can on Windows use for example PuTTY or msys2, and the integrated terminal and the <code>ssh</code> command itself directly if on Linux or macOS (also Windows). Type <code>ssh email@hostname</code> like above to connect immediately or just the hostname if you've added them to the SSH config file.</p>"},{"location":"access/ssh/#transferring-files","title":"Transferring files","text":"<p>Both VS Code and MobaXterm support file transfers, but you can also use other GUI apps like FileZilla or WinSCP. When just using a terminal there are several tools like <code>scp</code>, <code>rsync</code>, <code>rclone</code>, or <code>sftp</code>, all of which connect through the SSH protocol. You can also browse and transfer files through the interactive web portal described on the next page.</p>"},{"location":"access/ssh/#external-access","title":"External access","text":"<p>To access the servers while not directly connected to any AAU or eduroam network there are two options. Either you connect through VPN, which will route all your traffic through AAU, or you can use the SSH gateway through <code>sshgw.aau.dk</code> for SSH connections only. If you need virtual desktop access only VPN will work (for now).</p>"},{"location":"access/ssh/#vpn","title":"VPN","text":"<p>The simplest way is to connect to the AAU VPN using the guide provided by ITS here. After you connect everything should work as though you were at AAU.</p>"},{"location":"access/ssh/#using-an-ssh-jump-host","title":"Using an SSH jump host","text":"<p>The SSH gateway is simply a server hosted at AAU whose only purpose is to bridge SSH connections from the outside (open port 22 ingress) to something else on the internal AAU network. Setting up 2-factor authentication is required in order to connect. To enable the \"proxy jumping\" you need to adjust the SSH config file by uncommenting the <code>ProxyJump sshgw.aau.dk</code> line under the <code>*.srv.aau.dk</code> host, see the SSH config template file. Keep in mind that as long as it's enabled it will always connect through <code>sshgw.aau.dk</code> regardless of which network you are connected to, including the AAU network.</p>"},{"location":"access/ssh/#additional-configuration-optional","title":"Additional configuration (optional)","text":""},{"location":"access/ssh/#ssh-config-file","title":"SSH config file","text":"<p>To avoid typing hostnames and user names constantly here's a template SSH config file that includes all current servers. The file must be saved at certain locations depending on your OS. On Windows it's here, and on macOS and Linux it's usually under <code>~/.ssh/config</code>. Hostnames in the file will then be auto-completed and if you've set up SSH public key authentication you won't need to type anything else than for example <code>ssh bio-oscloud02.srv.aau.dk</code> and you're in.</p>"},{"location":"access/ssh/#ssh-config-file-template","title":"SSH config file template","text":"<pre><code># nice-to-have global options that apply to all servers,\n# prevents disconnects on network glitches and wifi reconnects,\n# allows forwarding GUI apps to client\nHost *\n  ServerAliveInterval 60\n  ServerAliveCountMax 2\n  ExitOnForwardFailure yes\n  ForwardX11 yes\n  ForwardX11Trusted yes\n\n# this is only used for external access\nHost sshgw.aau.dk\n  HostName sshgw.aau.dk\n  User abc@bio.aau.dk\n  Port 22\n  ForwardAgent yes\n\n# use the same user name (and optional SSH key) for all bioservers\n# uncomment the ProxyJump line for external access without VPN\nHost *.srv.aau.dk bio-os*\n  User abc@bio.aau.dk\n  Port 22\n  IdentityFile ~/.ssh/keys/bioservers\n#  ProxyJump sshgw.aau.dk\n\nHost bio-ospikachu01.srv.aau.dk\nHost bio-ospikachu02.srv.aau.dk\nHost bio-ospikachu03.srv.aau.dk\n</code></pre>"},{"location":"access/ssh/#ssh-public-key-authentication","title":"SSH Public Key Authentication","text":"<p>SSH public key authentication offers a more secure way to connect to a server, and is also more convenient, since you don't have to type in your password every single time you log in or transfer a file. An SSH private key is essentially just a very long password that is used to authenticate with a server holding the cryptographically linked public key for your user (think of it as the lock for the private key). You can even add an additional layer of security by encrypting the private key itself using a password when generating the pair. Any SSH client that you choose to use will connect through the SSH program on your computer under the hood, so public key authentication will also apply to them if set up like below.</p>"},{"location":"access/ssh/#generating-ssh-key-pairs","title":"Generating SSH Key Pairs","text":"<p>This must be done locally for security reasons, so that the private key never leaves your computer. If you use a password manager (please do) like 1Password or bitwarden you can usually both generate and safely store and use SSH keys directly from the vault without it lying around in a file. It's important that the key is not generated using the default (usually) RSA type algorithm, because it's outdated and can be brute-forced easily with modern hardware, so please use for example the <code>ed25519</code> algorithm instead.</p> <ol> <li>Open your terminal (on Windows called command prompt, hit <code>win+r</code> and type <code>cmd</code>)</li> <li>Generate an SSH key pair by running the command: <code>ssh-keygen -t ed25519</code></li> <li>Follow the prompts, and save the two keyfiles somewhere (the convention is to place it somewhere in the hidden folder <code>~/.ssh/</code> in your home folder)</li> </ol>"},{"location":"access/ssh/#adding-public-keys-to-the-server","title":"Adding Public Keys to the Server","text":"<p>Copy your public key to the server using <code>ssh-copy-id -i ~/.ssh/biocloud.pub username@hostname</code>, or manually add the contents of the public key file into the <code>~/.ssh/authorized_keys</code> file on any of the servers (the home folder is shared across them all). SSH requires that your user is the only one able to read the contents before you are allowed to login, so ensure the correct permissions are set using <code>chmod 700 ~/.ssh &amp;&amp; chmod 600 ~/.ssh/authorized_keys</code>.</p> <p>You should now be able to login securely without typing a password - have fun!</p>"},{"location":"access/webportal/","title":"Interactive Web portal (OpenOndemand)","text":"<p>Another way to access the BioCloud compute resources is to use the interactive web portal based on Open Ondemand developed by Ohio Supercomputer Center. The main purpose of the web portal is to greatly simplify running interactive and graphical apps such as virtual desktops and IDEs, which are served directly from isolated SLURM jobs running on the compute nodes, but you can also easily browse around and transfer files, obtain a shell, check the job queue, or compose job batch scripts based on templates, and more - all from your browser without having to learn a multitude of terminal commands.</p> <p>This page only describes briefly how to connect to the web portal. Guides detailing how to use all of its features are available under the Guides section. It is also absolutely essential to read and understand how SLURM and resource management works (described on the next page), before using any of the apps, because interactive apps are often terribly inefficient (i.e. CPUs do nothing when you are just typing or clicking around)!</p>"},{"location":"access/webportal/#getting-access","title":"Getting access","text":"<p>In order to access the web portal you must first be connected to the local AAU campus network, or through VPN. Then go to https://biocloud.bio.aau.dk and log in using your usual AAU credentials if you aren't already (for example if you've already signed in to your AAU webmail, etc). For security reasons the website is NOT exposed to the public internet, hence your browser will likely warn you that the site is insecure because the SSL certificate can't be validated by an authority because it can't access it:</p> <p></p> <p>But don't worry, traffic is still encrypted. To solve this, depending on your browser, just click Advanced and then proceed to biocloud.bio.aau.dk (unsafe):</p> <p></p> <p>If this is the first time you ever log in to any of the BioCloud resources, your home directory does not yet exist on the network storage and you will likely see an error. To create your home directory you must log in just once either through SSH as described on the previous page or using the web portal shell, which is available here. You can also just contact an admin. Now you should be greeted by the front page:</p> <p></p> <p>Now, please read through the SLURM guide on the following pages before using any of the apps. Afterwards you can go through the guides for the individual apps under the Guides section.</p>"},{"location":"guides/git/","title":"Setting up Git/GitHub","text":""},{"location":"guides/git/#repository-specific-deploy-keys","title":"Repository specific deploy keys","text":"<p>https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys#deploy-keys</p>"},{"location":"guides/sra/","title":"Downloading data from NCBI SRA","text":"<p>https://bioinformatics.ccr.cancer.gov/docs/b4b/Module1_Unix_Biowulf/Lesson6/</p>"},{"location":"guides/sra/#download-sra-tools-container","title":"Download sra-tools container","text":"<pre><code>singularity pull docker://ncbi/sra-tools\n</code></pre>"},{"location":"guides/sra/#get-data","title":"Get data","text":"<p>bioproject: PRJNA192924 sra: SRR1154613 prefetch first, then use fasterq-dump <pre><code>singularity run sra-tools_latest.sif prefetch SRR1154613\nsingularity run sra-tools_latest.sif fasterq-dump --threads $(nproc) --progress --split-3 SRR1154613\n</code></pre></p>"},{"location":"guides/sra/#download-sra-data-from-ena-instead","title":"Download SRA data from ENA instead","text":"<p>https://www.ebi.ac.uk/ena/browser/view/SRR1154613</p>"},{"location":"guides/sshdslurm/","title":"Interactive VS Code session in a SLURM job","text":"<p>When using the Visual Studio Code IDE to work remotely, you would normally install the Remote - SSH on your local computer and connect to the particular machine over an SSH connection to work there instead. This is useful when you for example need to work closer to raw data to avoid moving large data around, or when you simply need more juice for demanding tasks. In order to use VS Code remotely on a SLURM cluster, however, you can't normally connect directly to a compute node, and connecting directly to login nodes to run things there is strictly forbidden. This small guide will show you how you can connect your VS Code session directly to a SLURM job running on a compute node instead.</p> Important <p>Please note that this should be considered an interactive job and the SLURM job should therefore be submitted to a partition that has over-subscription set up, where CPU's are shared, because you will likely spend time typing instead of actually keeping the allocated CPU's busy at all times, which is a waste of resources.</p>"},{"location":"guides/sshdslurm/#how-does-it-work","title":"How does it work?","text":"<p>When you normally connect to a remote server, you use an SSH client to connect to the SSH daemon <code>sshd</code>, which is a service running in the background on the server. You then simply talk to this daemon using the SSH protocol to execute commands on the server. The trick here is then to start a separate <code>sshd</code> process that runs within a SLURM job, which you then connect to instead through a bridge connection to the job through one of the login nodes.</p>"},{"location":"guides/sshdslurm/#slurm-job-script","title":"SLURM job script","text":"<p>Log in through SSH on one of the login nodes, copy the following SLURM batch script somewhere, and adjust the resource requirements for your session. Submit the job using <code>sbatch</code> as usual, and remember to cancel it when you are done. It won't stop when you close the VS Code window on your computer unless it runs out of time. You will have to submit a job like this every time you want to use VS Code interactively (for anything else than code editing and file browsing etc). </p> <pre><code>#!/bin/bash\n#SBATCH --output=/dev/null\n#SBATCH --job-name=sshdbridge\n#SBATCH --time=0-3:00:00\n#SBATCH --partition=shared\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=6G\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# exit on first error or unset variable\nset -eu\n\n# find open port (open and close a socket, and reuse the port)\nPORT=$(python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\nscontrol update JobId=\"$SLURM_JOB_ID\" Comment=\"$PORT\"\n\n# check whether SSH host key exists (used for sshd host authentication, NOT for user authentication)\nssh_host_key=\"ssh_host_ed25519_key\"\nif [[ ! -s \"${HOME}/.ssh/${ssh_host_key}\" ]]\nthen\n  mkdir -p \"${HOME}/.ssh/\"\n  ssh-keygen -t ed25519 -N \"\" -f \"${HOME}/.ssh/${ssh_host_key}\"\n  chmod 600 \"${HOME}/.ssh/${ssh_host_key}\"\nfi\n\n# start sshd server on the available port\necho \"Starting sshd on port $PORT\"\n/usr/sbin/sshd -D -p \"${PORT}\" -h \"${HOME}/.ssh/ssh_host_ed25519_key\"\n</code></pre>"},{"location":"guides/sshdslurm/#adjust-local-ssh-config","title":"Adjust local SSH config","text":"<p>On your local computer, you need to set up a bridge connection through a login node by adding a few lines to your SSH config file. You will only have to do this once. You can find it through VS Code under the \"Remote Explorer\" side menu by clicking the little cog \"Open SSH Config File\":</p> <p></p> <p>Then add the following line somewhere:</p> <p>Windows <pre><code>Host bio-ospikachu02-sshdbridge\n    ProxyCommand ssh bio-ospikachu02.srv.aau.dk bash -c \\\"nc \\$(squeue --me --name=sshdbridge --states=R -h -O NodeList,Comment)\\\"\n</code></pre></p> <p>Linux/MacOS <pre><code>Host bio-ospikachu02-sshdbridge\n    ProxyCommand ssh bio-ospikachu02.srv.aau.dk \"nc \\$(squeue --me --name=sshdbridge --states=R -h -O NodeList,Comment)\"\n</code></pre></p> <p>In this example, the hostname of the login node <code>bio-ospikachu02.srv.aau.dk</code> must already exist in your SSH config file for it to work. You can use the provided SSH config template if you haven't added any BioCloud hosts there yet, and you can also use any other login node. Save the file and you should now see the new bridge host under \"Remote Explorer\" (you may need to hit refresh first):</p> <p></p> <p>Finally, click the \"Connect in New (or current) Window\" icon next to the name. If all goes well you should see the name of the login node (as it's named in your SSH config) in the bottom left corner:</p> <p></p> <p>Now you can start working! Whatever you do in VS Code now will run remotely inside a SLURM job on one of the compute nodes with a connection through the login node. Magic.</p>"},{"location":"guides/sshdslurm/#notes","title":"Notes","text":"<ul> <li>You will not be able to connect if you use an SSH jump host. Connect through VPN instead if you are not at AAU.</li> <li>You can have multiple simultaneous connections to the same job, however if there are different resource requirements for each task you need to do, you must submit individual jobs and use a different name for each job, and also create separate entries in the SSH config for each job.</li> </ul>"},{"location":"guides/vscode/","title":"Vscode","text":"<p>By default, VSCode wants to help you write code. So much so that it starts to get intrusive. When you type, the IntelliSense kicks in and immediately suggests code for you upon each keystroke. When you hover over your code, a popup definition appears. When you type an open parenthesis, it pops up another autocomplete suggestion window. When you type out a tag, a closing tag magically appears, but sometimes it's wrong. I get what VSCode is trying to do but it got to a point where it was annoying me and getting in the way.</p> <p>If you want VSCode to become more of a passive editor; if you enjoy typing the majority or all of your code, then add these to your settings.json to disable these autocomplete suggestions and pop ups:</p> <pre><code>\"editor.autoClosingBrackets\": \"never\",\n\"editor.suggestOnTriggerCharacters\": false,\n\"editor.quickSuggestions\": false,\n\"editor.hover.enabled\": false,\n\"editor.parameterHints.enabled\": false,\n\"editor.suggest.snippetsPreventQuickSuggestions\": false,\n\"html.suggest.html5\": false\n</code></pre> <p>Experiencing problems with vscode, hanging on remote connection etc. Log in on the particular login node and kill all vscode-server processes: <pre><code>ps ux | grep [.]vscode-server | awk '{print $2}' | xargs kill\n</code></pre></p> <p>then try again.</p>"},{"location":"guides/snakemake/biocloud/","title":"Running Snakemake workflows on SLURM clusters","text":"<p>Please first ensure you understand the basics of submitting SLURM jobs before running Snakemake workflows (or anything else!) on the BioCloud. It's highly recommended that you use a profile like the one provided below to properly allow Snakemake to start tasks as individual SLURM jobs and not run Snakemake itself in a large resource allocation (=job). Snakemake itself hardly requires any resources, 1CPU and 1GB memory is plenty. It's always important when developing Snakemake workflows to make sure that reasonable resource requirements are defined for the individual rules in the workflow (listed under <code>resources:</code> and <code>threads:</code>). Then it's only a matter of letting Snakemake be aware that it's being used on a HPC cluster and it will do things properly for you.</p>"},{"location":"guides/snakemake/biocloud/#dry-run-for-inspection","title":"Dry run for inspection","text":"<p>Before running the workflow, the DAG visualization mentioned on the previous page is a very useful way to quickly get an overview of exactly which tasks will be run and the dependencies between them. It can become quite large though, so it can also be useful to perform a \"dry run\", where Snakemake will output all of the tasks to be run without actually running anything. This can be done in a small interactive job and the output piped to a file with fx:</p> <pre><code>srun --ntasks 1 --cpus-per-task 1 --mem 1G snakemake -n &gt; workflow_dryrun.txt\n</code></pre> <p>If you have used the recommended folder structure mentioned earlier, Snakemake will automatically detect and use the <code>workflow/Snakefile</code>, but you may have more than one, in which case you must also supply <code>-s &lt;path to Snakefile&gt;</code>. </p>"},{"location":"guides/snakemake/biocloud/#submit-the-workflow","title":"Submit the workflow","text":"<p>When you have inspected the DAG or output from the dry run and you are ready to submit the full-scale workflow, you can do this in a non-interactive SLURM job using a batch script like this one:</p> <pre><code>#!/usr/bin/bash -l\n#SBATCH --job-name=&lt;snakemake_template&gt;\n#SBATCH --output=job_%j_%x.out\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1-00:00:00\n#SBATCH --partition=shared\n#SBATCH --mem=1G\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# Exit on first error and if any variables are unset\nset -eu\n\n# Activate conda environment with only snakemake\nconda activate &lt;snakemake_template&gt;\n\n# Start workflow using resources defined in the profile. Snakemake itself \n# requires nothing, 1CPU + 1G mem is enough\n\n# Render a DAG to visualize the workflow (optional)\nsnakemake --dag | dot -Tsvg &gt; results/dag.svg\n\n# Main workflow\nsnakemake --profile biocloud\n\n# Generate a report once finished (optional)\nsnakemake --report results/report.html\n</code></pre> <p>From this job Snakemake will submit individual SLURM jobs on your behalf for each task with the resources defined for each rule. This can sometimes start hundreds of jobs (of course within your limits) depending on the workflow and data - so please ensure you have defined a reasonable amount of resources for each rule and that the individual tasks utilize them as close to 100% as possible, especially CPUs. Often resource requirements depend on the exact input data or database being used, so it's also possible to dynamically scale the requirements using simple python lambda functions to calculate appropriate values for each resource, see Snakemake docs for details.</p>"},{"location":"guides/snakemake/biocloud/#biocloud-snakemake-profile","title":"BioCloud Snakemake profile","text":"<p>When using Snakemake to submit SLURM jobs the command with which to start the workflow can easily become quite long (as many as 20+ arguments). Using Snakemake profiles instead is an easy way to avoid this, where any command line options are simply written to a <code>config.yaml</code> file instead, which is then read by Snakemake using the <code>--profile</code> argument. On the BioCloud login and compute nodes there is already an optimized default profile pre-installed for all users in <code>/etc/xdg/snakemake/biocloud/config.yaml</code>, which will be automatically found by snakemake by name if you just type <code>snakemake --profile biocloud</code>. If you need to adjust this profile to suit your needs, you must copy it and place it in either the project folder from where snakemake will be run, or in the user standard location <code>~/.config/snakemake/biocloud/config.yaml</code> to avoid copying it around several times for each project. You must supply a name or a path to a folder in which a <code>config.yaml</code> is placed, not the path to the file itself. The default profile, which is optimized for our particular setup, is also shown below.</p> <pre><code>#command with which to submit tasks as SLURM jobs\ncluster:\n  mkdir -p logs/{rule}/ &amp;&amp;\n  sbatch\n    --parsable\n    --partition={resources.partition}\n    --qos={resources.qos}\n    --cpus-per-task={threads}\n    --mem={resources.mem_mb}\n    --gpus={resources.gpus}\n    --time={resources.runtime}\n    --job-name=smk-{rule}-{wildcards}\n    --output=logs/{rule}/{rule}-{wildcards}-%j.out\n#if rules don't have resources set, use these default values.\n#Note that \"mem\" will be converted to \"mem_mb\" under the hood, so mem_mb is prefered\ndefault-resources:\n  - partition=\"shared\"\n  - qos=\"normal\"\n  - threads=1\n  - mem_mb=512\n  - gpus=0\n  - runtime=\"0-01:00:00\"\n#max threads per job/rule. Will take precedence over anything else. Adjust this\n#before submitting to SLURM and leave threads settings elsewhere untouched\nmax-threads: 32\nuse-conda: False\nuse-singularity: False\nprintshellcmds: False\njobs: 50\nlocal-cores: 1\nlatency-wait: 120\nrestart-times: 0\nmax-jobs-per-second: 10 #don't touch\nkeep-going: True\nrerun-incomplete: True\nscheduler: greedy\nmax-status-checks-per-second: 5\ncluster-cancel: scancel\n#script to get job status for snakemake, unfortunately neccessary\ncluster-status: /etc/xdg/snakemake/biocloud/slurm-status.sh\n</code></pre> <p>Imagine writing all these settings on the command line every time - just no!</p> <p>The <code>slurm-status.sh</code> script on the last line is available from the template repository here or at <code>/etc/xdg/snakemake/biocloud/slurm-status.sh</code>.</p>"},{"location":"guides/snakemake/intro/","title":"Getting Started","text":""},{"location":"guides/snakemake/intro/#introduction","title":"Introduction","text":"<p>Snakemake is a workflow management system that simplifies the specification and execution of complex data processing and analysis pipelines using both self-made rules (tasks or steps) and/or reusable community-made wrappers. It's an open-source tool, designed to scale from simple to highly complex workflows, providing both flexibility and reproducibility for any computing environment. In comparison to shell scripts Snakemake is much more efficient because it automatically calculates rule dependencies between the individual rules depending on expected inputs and outputs as well as the input data itself, which greatly simplifies parallel computing. Snakemake workflows are written in Python in a standardized format, but has full support for any scripting language. It also has advanced features that automates the installation of required software tools ensuring complete portability and compatibility. Last, but not least, Snakemake has inherent support for cluster execution where workflow steps are translated and submitted as separate batch jobs with individually specified resource requirements to maximize resource utilization, while also managing the submission process, job completion monitoring, and much more.</p> <p>Snakemake provides detailed (and quite messy) documentation at snakemake.readthedocs.io, however the purpose of this guide is to boil things down a bit to help you get started on the BioCloud HPC for common use-cases and provide a standardized starting point for everyone. Let's get started.</p>"},{"location":"guides/snakemake/intro/#preparing-project-folder-for-snakemake","title":"Preparing project folder for Snakemake","text":"<p>The easiest way to get started with a new project is to create a new git repository on GitHub from our template GitHub repository. You of course need a GitHub account first, but that will also come in handy in many other situations. The template repository includes a minimal example workflow which will be explained on the next page, so that you can follow along. Clone the repo afterwards to your home folder somewhere and start filling in after this guide. According to the authors of Snakemake, the ideal way to structure a Snakemake project folder is like this (with a few additions):</p> <pre><code>project/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 analysis/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 config.yaml\n\u251c\u2500\u2500 data/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 profiles/\n\u251c\u2500\u2500 results/\n\u2514\u2500\u2500 workflow/\n    \u251c\u2500\u2500 envs/\n    \u2502   \u251c\u2500\u2500 tool1.yml\n    \u2502   \u2514\u2500\u2500 tool2.yml\n    \u251c\u2500\u2500 notebooks/\n    \u2502   \u251c\u2500\u2500 notebook1.ipynb\n    \u2502   \u2514\u2500\u2500 notebook2.ipynb\n    \u251c\u2500\u2500 report/\n    \u2502   \u251c\u2500\u2500 plot1.rst\n    \u2502   \u2514\u2500\u2500 plot2.rst\n    \u251c\u2500\u2500 rules/\n    \u2502   \u251c\u2500\u2500 rule1.smk\n    \u2502   \u2514\u2500\u2500 rule2.smk\n    \u251c\u2500\u2500 scripts/\n    \u2502   \u251c\u2500\u2500 script1.R\n    \u2502   \u251c\u2500\u2500 script2.sh\n    \u2502   \u2514\u2500\u2500 script3.py\n    \u2514\u2500\u2500 Snakefile\n</code></pre> <p>At first it might seem like a lot of files and folders, but workflows can grow quickly, so it's nice with a proper structure from the beginning. You can of course also put everything in a separate subfolder if developing a workflow is not the main goal of the project.</p>"},{"location":"guides/snakemake/intro/#installation","title":"Installation","text":"<p>To setup Snakemake use the <code>environment.yaml</code> file provided in the template repository to create a conda environment from a file for the project with <code>conda env create -f environment.yml</code>. It's always best practice to note software dependencies down somewhere in a file, but alternatively you can also load the prebuilt software module using: <pre><code>module load snakemake/7.18.2-foss-2020b\n</code></pre></p> <p>or create and activate a conda environment for the project on the command line: <pre><code>conda create -c conda-forge -c bioconda -n snakemake snakemake==7.18.2\nconda activate snakemake\n</code></pre></p> Note: The Snakemake version matters <p>Snakemake keeps track of files but also the Snakemake version used to run the workflow. Therefore it is important to use the same version of snakemake when re-running a workflow. If you try to run a workflow with a different version of snakemake, it will re-run all samples already processed. Therefore be explicit about the version installed and ensure that it's written down somewhere, fx in the project <code>README</code> file or a conda <code>environment.yml</code> file.</p> <p>If you use Visual Studio Code, it can be handy to install the <code>snakemake</code> extension to enable syntax highlighting, linting, and autocompletion when developing workflows.</p>"},{"location":"guides/snakemake/intro/#workflow-catalogs","title":"Workflow catalogs","text":"<p>Before you go ahead and create a new workflow from scratch, it's a good idea to have a look in the public Snakemake workflow catalog, because someone might have already made something similar you could use instead. Also have a look at WorkflowHub for workflows made with Snakemake, but also other workflow tools.</p> <p>The template repository is also configured with a few standard GitHub Actions to automatically test the workflow, and passing some tests is required if you also wish to publish it to the catalog. As long as your repository is public and the <code>README.md</code> file contains the two words snakemake and workflow (and a few other rules for inclusion) it will show up in the catalogue automatically. You don't have to do anything specific for it to be listed there, they simply scan GitHub at regular intervals. This is the best way to share your workflow for others to use, but it's optional.</p> <p>Lastly, if you stick to the standardized folder structure above and keep the workflow in a GitHub repository (public or private) you can easily reuse your workflow across multiple projects using snakedeploy. This is also how workflows are reused from the catalog.</p>"},{"location":"guides/snakemake/tutorial/","title":"Workflow tutorial","text":"<p>To demonstrate how Snakemake is used in practice we'll go through an example workflow. To be able to follow along it's assumed that you have set up a project folder from the template git repository as described on the previous page.</p>"},{"location":"guides/snakemake/tutorial/#example-workflow","title":"Example workflow","text":"<p>The goal of this simple example workflow is to map some demultiplexed nanopore sequences to a database. This is done in two steps, which are called rules in Snakemake. The rules will do the following:</p> <ul> <li>Concatenate all <code>.fastq.gz</code> files in each subfolder in the input data folder into single files using the subfolder (barcode) names as sample IDs</li> <li>Map each file to a database using <code>minimap2</code> and <code>samtools</code> to produce a <code>.sam</code> file for each sample</li> </ul>"},{"location":"guides/snakemake/tutorial/#the-snakefile","title":"The <code>Snakefile</code>","text":"<p>The <code>workflow/Snakefile</code> is the main workflow file where all inputs and outputs are defined as well as the actual rules to run. To achieve our goal a <code>Snakefile</code> could look like this:</p> <p>workflow/Snakefile <pre><code>import os\nfrom snakemake.utils import min_version\n\n# minimum snakemake version required\nmin_version(\"7.18.2\")\n\n# config file path\nconfigfile: \"config/config.yaml\"\n\n# list all subfolders in the input_dir (defined in the config.yaml file)\nsample_dirs = os.listdir(config['input_dir'])\n\n# include rules\ninclude: \"rules/concatenate_fastq.smk\"\ninclude: \"rules/map2db.smk\"\n\n# define the expected output using wildcards\nrule all:\n  input:\n    expand(os.path.join(config['output_dir'], \"{sample}.sam\"), sample=sample_dirs)\n</code></pre> The <code>Snakefile</code> must always contain the special rule <code>all</code>. This rule is used to define a list with the expected output (confusingly listed under <code>input</code>) of the entire workflow, which can be dynamically generated depending on the input data using wildcards and the <code>expand()</code> function, which in this example will generate a list of output <code>.sam</code> files for each subfolder in the <code>input_dir</code> defined in the config file. Learning how to use wildcards is very important as it's required to enable many of the awesome features Snakemake provides, so it's highly recommended experimenting a bit with them and read the Snakemake docs.</p>"},{"location":"guides/snakemake/tutorial/#workflow-configuration-file","title":"Workflow configuration file","text":"<p>The <code>configfile</code> variable is a required Snakemake variable and must point to a config file, which in this example looks like this:</p> <p>config/config.yaml <pre><code>output_dir: \"results\"\ninput_dir: \"data/samples/\"\ntmp_dir: \"tmp\"\ndb_path: \"/databases/midas/MiDAS5.2_20231221/output/FLASVs.fa\"\nmax_threads: 128\n</code></pre></p> <p>The config file can contain anything, and is used to allow the user to customize how the workflow will run, which input and database files to use, individual settings for certain tools, and so on. Importantly, it should NOT contain any settings relevant for the exact computing setup and how things are run on a particular platform. For this Snakemake instead uses profiles that are supposed to be configured in <code>config.yaml</code> files under <code>profiles/</code> instead, more on them on the next page.</p>"},{"location":"guides/snakemake/tutorial/#rules","title":"Rules","text":"<p>Rules define the actual workflow steps and from these Snakemake will dynamically start one or more tasks depending on the input data and configuration. The two rules in the above example <code>Snakefile</code> are placed in two files separate from the <code>Snakefile</code> and imported using <code>include</code> statements to provide a better overview as the workflow grows, but they could also have been written directly in the <code>Snakefile</code> itself. The files can each contain any number of rules or arbitrary Python code. The two files for the workflow look like this:</p> <p>concatenate_fastq.smk <pre><code>import glob\nimport os\n\n# helper function to generate a list of all fastq.gz files \n# per wildcard (subfolder/sample).\n# See https://snakemake.readthedocs.io/en/latest/tutorial/advanced.html#step-3-input-functions\ndef listFastq(wildcards):\n  fastqs = glob.glob(os.path.join(config['input_dir'], wildcards.sample, \"*.fastq.gz\"))\n  return fastqs\n\nrule concatenate_fastq:\n  input:\n    listFastq\n  output:\n    temp(os.path.join(config['tmp_dir'], \"samples\", \"{sample}_concat.fastq.gz\"))\n  resources:\n    # calculate required memory based on input file size\n    # assuming it scales linearly, ensures a minimum of 512MB\n    mem_mb=lambda wc, input: max(3 * input.size_mb, 512)\n  threads: 1\n  log:\n    os.path.join(config[\"log_dir\"], \"concatenate_fastq\", \"{sample}.log\"),\n  shell:\n    \"cat {input} &gt; {output}\"\n</code></pre></p> <p>map2db.smk <pre><code>rule map2db:\n  input:\n    os.path.join(config['tmp_dir'], \"samples\", \"{sample}_concat.fastq.gz\")\n  output:\n    os.path.join(config['output_dir'], \"{sample}.sam\")\n  resources:\n    # calculate required memory based on input file size\n    # assuming it scales linearly, ensures a minimum of 10GB\n    mem_mb=lambda wc, input: max(3 * input.size_mb, 10240)\n  threads: config['max_threads']\n  params:\n    db_path = config['db_path']\n  conda:\n    \"../envs/map2db.yml\"\n  log:\n    os.path.join(config[\"log_dir\"], \"map2db\", \"{sample}.log\")\n  shell:\n    \"\"\"\n    minimap2 \\\n      -ax map-ont \\\n      -K20M \\\n      -t {threads} \\\n      --secondary=no \\\n      {params.db_path} \\\n      {input} \\\n      | samtools \\\n        view \\\n        -F 4 \\\n        -F 256 \\\n        -F 2048 \\\n        --threads $(nproc) \\\n        -o {output}\n    \"\"\"\n</code></pre></p> <p>Rules can also run any arbitrary scripts placed in the <code>workflow/scripts</code> folder instead of shell commands, which can be handy for more customization and longer commands, or to run R scripts and markdown files to produce some figures, perform analyses, and so on. Refer to the Snakemake docs for details about how to pass on variables from the Snakemake workflow to the scripts.</p>"},{"location":"guides/snakemake/tutorial/#example-output","title":"Example output","text":"<p>For now, it's more important that you understand the essential components of a workflow and what the resulting output from the above example looks like before learning how to properly run a Snakemake workflow on the BioCloud HPC. This will be described on the next page instead.</p> <p>When running the workflow above the <code>concatenate_fastq</code> rule will with the following example input data:</p> <pre><code>data/samples\n\u251c\u2500\u2500 barcode01\n\u2502   \u251c\u2500\u2500 FAW32656_pass_barcode01_06cd0bbc_287fe392_0.fastq.gz\n\u2502   \u2514\u2500\u2500 FAW32656_pass_barcode01_06cd0bbc_287fe392_1.fastq.gz\n\u251c\u2500\u2500 barcode02\n\u2502   \u251c\u2500\u2500 FAW32656_pass_barcode02_06cd0bbc_287fe392_0.fastq.gz\n\u2502   \u2514\u2500\u2500 FAW32656_pass_barcode02_06cd0bbc_287fe392_1.fastq.gz\n\u2514\u2500\u2500 barcode03\n    \u251c\u2500\u2500 FAW32656_pass_barcode03_06cd0bbc_287fe392_0.fastq.gz\n    \u2514\u2500\u2500 FAW32656_pass_barcode03_06cd0bbc_287fe392_1.fastq.gz\n</code></pre> <p>start 3 separate tasks that result in the following (temporary) output files: <pre><code>tmp\n\u2514\u2500\u2500 samples\n    \u251c\u2500\u2500 barcode01_concat.fastq.gz\n    \u251c\u2500\u2500 barcode02_concat.fastq.gz\n    \u2514\u2500\u2500 barcode03_concat.fastq.gz\n</code></pre></p> <p>As soon as any of the files have been produced by the 3 separate and totally independent <code>concatenate_fastq</code> tasks, the <code>map2db</code> tasks will start immediately regardless of whether all files have been produced first - no time lost. From the 3 temporary files the <code>map2db</code> rule will spawn 3 new separate tasks and produce the following files:</p> <pre><code>results\n\u251c\u2500\u2500 barcode01.sam\n\u251c\u2500\u2500 barcode02.sam\n\u2514\u2500\u2500 barcode03.sam\n</code></pre> <p>This is also the final output defined in the <code>all</code> rule, so once these 3 files have been created, Snakemake exits and we're done.</p> <p>If we now run the workflow again, Snakemake won't do anything unless specifically asked to, because the expected final output has already been produced previously. The same is true for the individual tasks and this is how Snakemake adds checkpoints to workflows. It will not start from scratch every time you run a workflow, but instead move on from the last checkpoint until the final output has been produced, which is very handy if (when) something fails.</p>"},{"location":"guides/snakemake/tutorial/#the-directed-acyclic-graph-dag","title":"The Directed Acyclic Graph (DAG)","text":"<p>The <code>Snakefile</code> will be read sequentially by Snakemake, however the exact order of the individual rules doesn't matter, since Snakemake will, before running anything, first build a dependency graph called the DAG (Directed Acyclic Graph) between all the rules depending on the <code>input</code>'s and <code>output</code>'s defined for each rule and the exact input data used. Some rules can therefore be run completely independently of eachother in parallel if they don't depend on eachother like in the example workflow above, while other rules will not run before an expected output from another rule has been produced first. This is a very powerful feature that is well-suited for HPC systems that use a job scheduling system like SLURM, because the tasks can be submitted by Snakemake as separate jobs with separate resource allocations and run simultaneously, while still under Snakemake's control. The workflow DAG can be visualized using the following command, and will for this particular example data with 3 samples look like this:</p> <pre><code>snakemake --dag | dot -Tsvg &gt; dag.svg\n</code></pre> <p></p> <p>If we now need to scale this up to a full nanopore flowcell with 96 barcodes, for example, Snakemake would simply submit 96 jobs at once instead of 3, while the time it takes to finish would be the same (of course as long as there are resources available). Awesome.</p>"},{"location":"guides/snakemake/tutorial/#software-deployment-methods","title":"Software deployment methods","text":"<p>To ensure the required software is available for each rule Snakemake supports both Conda environments and apptainer/singularity containers. You can add support for both at the same time by defining both <code>conda:</code> and <code>container:</code> at each rule definition, and then let the user decide which one to use.</p>"},{"location":"guides/snakemake/tutorial/#conda","title":"Conda","text":"<p>To use Conda environments you simply note the requirements down in a file, preferably with the exact versions used during development, and provide the path to <code>conda</code> at the rule definition like in the <code>map2db</code> rule above. An environment file might look like this:</p> <p>envs/map2db.yml <pre><code>name: map2db\nchannels:\n  - bioconda\ndependencies:\n  - minimap2=2.26\n  - samtools=1.18\n</code></pre></p> <p>Snakemake will then create the environment(s) the first time the workflow is run and automatically load the environment before running each task.</p>"},{"location":"guides/snakemake/tutorial/#containers","title":"Containers","text":"<p>To use apptainer/singularity containers you just supply the name of the container in a container registry, for example <code>container: docker://biocontainers/minimap2</code>. If you need a container with multiple tools at once see multi-package containers.</p>"},{"location":"guides/snakemake/tutorial/#software-modules","title":"Software modules","text":"<p>Software environment modules are also supported by supplying the module name to <code>envmodules</code> in the rule definition. Modules are not as declarative and portable in nature as Conda environments or containers, however, and is therefore not recommended. See additional details here.</p>"},{"location":"guides/snakemake/tutorial/#snakemake-wrappers","title":"Snakemake wrappers","text":"<p>The above example is rather simple and is only made for demonstrating the basics of Snakemake workflows. But if this was part of a real workflow, however, it would not be the proper way to achieve our particular goals, because someone has of course already used minimap2 in a Snakemake workflow before. So we could have saved some time and effort and instead used one of the many community-made Snakemake wrappers for each rule. The format is exactly the same as a normal rule where you define <code>input</code>'s and <code>output</code>'s etc, but the exact command(s) that the rule would normally run is replaced with fx <code>wrapper: \"v3.3.4/bio/minimap2/aligner\"</code>. If rules for a certain tool isn't available already - you can contribute too!</p>"},{"location":"guides/webportal/files/","title":"File browser","text":"<p>Browsing around directories in the terminal can be quite inconvenient, especially when you need to move or transfer many files. Using the file browser in the web portal is much simpler. To browse and transfer files click Files at the menu bar and then the desired starting folder, for example your home directory:</p> <p></p> <p>Everything here is pretty self-explanatory. Click Upload to upload a file, Download to download a file, select multiple files and/or folders at once for bulk actions, etc. Here's a screenshot of how it looks:</p> <p></p>"},{"location":"guides/webportal/jobcomposer/","title":"Job composer","text":"<p>Guide coming soon...</p>"},{"location":"guides/webportal/jobqueue/","title":"Job queue","text":"<p>You can browse the SLURM job queue and see details of all queued and running jobs by clicking Jobs and then Active jobs:</p> <p></p> <p>By default you will only see your own jobs, so it might be empty at first. To see the entire queue and the running jobs from all users click Your Jobs at the top right and then select All Jobs:</p> <p></p> <p>You can now see a list of all queued and running jobs as well as recently completed jobs for each hardware partition (confusingly named Queue):</p> <p></p> <p>To see additional details of individual jobs click the little arrow to the left of each job:</p> <p></p>"},{"location":"guides/webportal/shell/","title":"Shell access","text":"<p>Obtain a shell in the browser by clicking Clusters and then biocloud Shell Access:</p> <p></p> <p>Enter your password when prompted:</p> <p></p> <p>And you should now see the message of the day and a prompt in the exact same way as when accessing through SSH:</p> <p></p> <p>You can even choose between different themes at the top right if you want.</p>"},{"location":"guides/webportal/apps/coder/","title":"Code server","text":"<p>Code Server is an open source alternative to Visual Studio Code provided by Coder, which can be run through a web browser. It's based on much of the same code base and has the exact same features overall, however there are minor differences. If you want the full VS Code experience you must follow this guide instead. This app will allow you to run a Code Server in a SLURM job and access it directly from your browser.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/coder/#starting-the-app","title":"Starting the app","text":"<p>Start by selecting the desired working directory where Code will start (you can also change this inside), and the amount of resources that you expect to use and for how long:</p> <p></p> <p>Now it's important to choose an appropriate hardware partition for your job. You almost always want to use the <code>shared</code> partition where CPUs are shared, however if you are sure that you will keep them busy for most of the duration by optimizing CPU efficiency, or if you need a lot of memory, you can go ahead and use other partitions. Otherwise, please just use the <code>shared</code> partition. If you need to use a specific node, for example if you need some fast and local scratch space, you can type the hostname in the Nodelist field, otherwise just leave it blank. Keep in mind that selecting individual compute nodes may result in additional queue time.</p> <p></p> <p>Lastly, you can give the job an appropriate name and choose when you would like to receive an email. Most users don't need to choose between different accounts, since your user will likely only belong to a single one, in which case just leave it as-is. Then click Launch!</p> <p></p>"},{"location":"guides/webportal/apps/coder/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen hardware partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the job has been granted a resource allocation the server needs a little while to start and you will see the status change to Starting</p> <p></p> <p>and when it finishes a button will appear to launch Code Server:</p> <p></p> <p>You can now start working:</p> <p></p>"},{"location":"guides/webportal/apps/coder/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. You can do that by clicking the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used.</p>"},{"location":"guides/webportal/apps/jupyter/","title":"Jupyter notebook","text":"<p>Jupyter Notebook is a web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience. This app will allow you to run a Jupyter Notebook server in a SLURM job and access it directly from your browser.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/jupyter/#starting-the-app","title":"Starting the app","text":"<p>Start by selecting the amount of resources that you expect to use and for how long:</p> <p></p> <p>Now it's important to choose an appropriate hardware partition for your job. You almost always want to use the <code>shared</code> partition where CPUs are shared, however if you are sure that you will keep them busy for most of the duration by optimizing CPU efficiency, or if you need a lot of memory, you can go ahead and use other partitions. Otherwise, please just use the <code>shared</code> partition. If you need to use a specific node, for example if you need some fast and local scratch space, you can type the hostname in the Nodelist field, otherwise just leave it blank. Keep in mind that selecting individual compute nodes may result in additional queue time.</p> <p></p> <p>Lastly, you can give the job an appropriate name and choose when you would like to receive an email. Most users don't need to choose between different accounts, since your user will likely only belong to a single one, in which case just leave it as-is. Then click Launch!</p> <p></p>"},{"location":"guides/webportal/apps/jupyter/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen hardware partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the job has been granted a resource allocation the server needs a little while to start and you will see the status change to Starting</p> <p></p> <p>and when it finishes a button will appear to launch the notebook:</p> <p></p> <p>You can now start working:</p> <p></p>"},{"location":"guides/webportal/apps/jupyter/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. You can do that by either clicking the Quit button inside the notebook server in the top right corner:</p> <p></p> <p>or click the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used.</p>"},{"location":"guides/webportal/apps/jupyter/#installing-python-packages-for-jupyter","title":"Installing Python packages for Jupyter","text":""},{"location":"guides/webportal/apps/jupyter/#from-pypi","title":"From PyPI","text":"<p>If the exact Python version isn't important to you, you can install packages from pipy using <code>pip</code> directly from a notebook using for example <code>!pip install matplotlib</code>. However, this will use the default python version installed on the system, which cannot be changed and will be upgraded from time to time, which can result in incompatibilities and break dependencies between packages after each upgrade. To manage both the python version and python packages it's better to use conda environments.</p>"},{"location":"guides/webportal/apps/jupyter/#using-conda-environments","title":"Using conda environments","text":"<p>You can use conda environments to manage software and python packages and make them available for Jupyter notebooks by installing <code>ipykernel</code> into any environment as a separate kernel:</p> <pre><code>conda activate myproject\nconda install ipykernel\npython -m ipykernel install --user --name myproject --display-name myproject\n</code></pre> <p>This will install the kernel into <code>${HOME}/.local/share/jupyter/kernels</code> with the chosen display name and make it visible from Jupyter. Select the new kernel either when creating a new notebook:</p> <p></p> <p>or when already working on a notebook by changing kernel from the menus:</p> <p></p> <p>In a similar way you can also run R code in Jupyter notebooks by installing the IRkernel into an environment: <pre><code>conda activate myproject\nconda install r-irkernel\nR -e \"IRkernel::installspec(name = 'myproject-Rkernel', displayname = 'myproject-Rkernel')\"\n</code></pre></p> <p>You should see the R logo when you've chosen the correct kernel:</p> <p></p> <p>If you need to install R packages, don't run any <code>install.packages()</code> commands, you must install R packages into the conda environment instead.</p>"},{"location":"guides/webportal/apps/rstudio/","title":"RStudio","text":"<p>RStudio is an integrated development environment (IDE) for R and Python. It includes a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging, and workspace management. This app will allow you to run an RStudio server in a SLURM job and access it directly from your browser.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/rstudio/#starting-the-app","title":"Starting the app","text":"<p>Start by selecting the desired R version and the amount of resources that you expect to use and for how long:</p> <p></p> <p>Now it's important to choose an appropriate hardware partition for your job. You almost always want to use the <code>shared</code> partition where CPUs are shared, however if you are sure that you will keep them busy for most of the duration by optimizing CPU efficiency, or if you need a lot of memory, you can go ahead and use other partitions. Otherwise, please just use the <code>shared</code> partition. If you need to use a specific node, for example if you need some fast and local scratch space, you can type the hostname in the Nodelist field, otherwise just leave it blank. Keep in mind that selecting individual compute nodes may result in additional queue time.</p> <p></p> <p>Lastly, you can give the job an appropriate name and choose when you would like to receive an email. Most users don't need to choose between different accounts, since your user will likely only belong to a single one, in which case just leave it as-is. Then click Launch!</p> <p></p>"},{"location":"guides/webportal/apps/rstudio/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen hardware partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the job has been granted a resource allocation the server needs a little while to start and you will see the status change to Starting</p> <p></p> <p>and when it finishes a button will appear to launch RStudio:</p> <p></p> <p>You can now start working:</p> <p></p>"},{"location":"guides/webportal/apps/rstudio/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. Before you do that, however, it's a good idea to first click the little red button inside RStudio in the top right corner to shut down the R session gracefully:</p> <p></p> <p>Then stop the job by clicking the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used.</p>"},{"location":"guides/webportal/apps/rstudio/#containerization","title":"Containerization","text":"<p>The RStudio server runs in the SLURM job from within a singularity/apptainer container, that is based on Rocker container images. This means that R packages installed from the RStudio app may not work with other R installations due to different base operating system packages, so the RStudio app uses a different R library location by default, which is located under <code>$HOME/R/rstudio-server/R_MAJOR_VERSION</code>.</p> <p>Furthermore, because RStudio is running in an isolated Linux container, you cannot for example load your usual conda environments or issue SLURM commands from the integrated terminal, etc. Only the Ceph network storage mount points are available at their usual locations, but everything else is otherwise completely isolated from the host on which the container runs.</p>"},{"location":"guides/webportal/apps/virtualdesktop/","title":"Virtual Desktop","text":"<p>You can start a virtual desktop on any compute node and access it directly through the browser without having to install and set up any software on your own computer first to access it. This is useful for running GUI software that doesn't run from a command line, but instead needs to show up in graphical windows where you can click around like you are used to on your personal computer.</p> <p>Before continuing, please first follow the guide to getting access to the OpenOndemand web portal.</p>"},{"location":"guides/webportal/apps/virtualdesktop/#starting-the-app","title":"Starting the app","text":"<p>Click the Virtual Desktop app icon on the front page or select it from the Interactive Apps menu bar at the top:</p> <p></p> <p>Start by selecting the amount of resources that you expect to use and for how long:</p> <p></p> <p>Now it's important to choose an appropriate hardware partition for your job. You almost always want to use the <code>shared</code> partition where CPUs are shared, however if you are sure that you will keep them busy for most of the duration by optimizing CPU efficiency, or if you need a lot of memory, you can go ahead and use other partitions. Otherwise, please just use the <code>shared</code> partition. If you need to use a specific node, for example if you need some fast and local scratch space or use licensed software (fx CLC), you can type the hostname in the Nodelist field, otherwise just leave it blank. Keep in mind that selecting individual compute nodes may result in additional queue time.</p> <p></p> CLC workbench <p>If you want to use the CLC workbench from a virtual desktop, you must use the node <code>axomamma</code>, because CLC is only licensed for this particular machine. Additionally, the maximum number of CPUs allowed per CLC instance is 64.</p> <p>Lastly, you can give the job an appropriate name and choose when you would like to receive an email. Most users don't need to choose between different accounts, since your user will likely only belong to a single one, in which case just leave it as-is. Then click Launch!</p> <p></p>"},{"location":"guides/webportal/apps/virtualdesktop/#accessing-the-app","title":"Accessing the app","text":"<p>When you've clicked Launch SLURM will immediately start finding a compute node with the requested amount of resources available, and you will see a Queued status. When the chosen hardware partition is not fully allocated this usually only takes a few seconds, however if it takes longer, you can check the job status and the reason why it's pending under the Jobs menu, or by using shell commands.</p> <p></p> <p>When the SLURM scheduler has granted an allocation for the job, you will see the status change to Running and a button will appear to launch the virtual desktop:</p> <p></p> <p>Ensure that the Image Quality slider is maximized. You don't need to adjust the Compression level unless you are on a poor network connection, in which case you can increase it. You can also share the session with other people, which can be handy for fx teaching. Now, click Launch, and you should see a desktop like this within a few seconds:</p> <p></p> <p>You can now start running anything you want, browse the Applications menu at the top left to find and launch your software.</p>"},{"location":"guides/webportal/apps/virtualdesktop/#stopping-the-app","title":"Stopping the app","text":"<p>When you are done with your work, it's important to stop the app to free up resources for other users. You can do that by either clicking Log Out inside the virtual desktop in the top right corner:</p> <p></p> <p>or click the red Cancel button under My Interactive Sessions, see the screenshots above.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used.</p>"},{"location":"slurm/accounting/","title":"Usage accounting and priority","text":"<p>All users belong to an account (usually their PI) where all usage is tracked on a per-user basis, but limitations and priorities can be set at a few different levels: at the cluster, partition, account, user, or QOS level. User associations with accounts rarely change, so in order to be able to temporarily request additional resources or obtain higher priority for certain projects, users can submit to different SLURM \"Quality of Service\"s (QOS). By default, all users can only submit jobs to the <code>normal</code> QOS with equal resource limits and base priority for everyone. Periodically users may submit to the <code>highprio</code> QOS instead, which has extra resources and higher priority (and therefore the usage is also billed 2x), however this must first be discussed among the owners of the hardware (PI's), and then you must contact an administrator to grant your user permission to submit jobs to it.</p>"},{"location":"slurm/accounting/#job-priority","title":"Job priority","text":"<p>When a job is submitted a priority value is calculated based on several factors, where a higher number indicates a higher priority in the queue. This does not impact running jobs, and the effect of prioritization is only noticable when the cluster is operating near peak capacity, or when the hardware partition to which the job has been submitted is nearly fully allocated. Otherwise jobs will usually start immediately as long as there are resources available and you haven't reached the maximum CPU's per user limit.</p> <p>Different weights are given to the individual priority factors, where the most significant ones are the account fair-share factor (described in more detail below) and the QOS, as described above. All factors are normalized to a value between 0-1, then weighted by an adjustable scalar, which may be adjusted occasionally depending on the overall cluster usage. Users can also be nice to other users and reduce the priority of their own jobs by setting a \"nice\" value using <code>--nice</code> when submitting for example less time-critical jobs. Job priorities are then calculated according to the following formula:</p> <pre><code>Job_priority =\n    (PriorityWeightQOS) * (QOS_factor)\n    (PriorityWeightAge) * (age_factor) +\n    (PriorityWeightFairshare) * (fair-share_factor) +\n    (PriorityWeightJobSize) * (job_size_factor) -\n    - nice_factor\n</code></pre> <p>To obtain the current configured weights use <code>sprio -w</code>: <pre><code>$ sprio -w\n  JOBID PARTITION   PRIORITY       SITE        AGE  FAIRSHARE    JOBSIZE        QOS\nWeights                               1        100        800       904        1000\n</code></pre></p> <p>The priority of pending jobs will be shown in the job queue when running <code>squeue</code>. To see the exact contributions of each factor to the priority of a pending job use <code>sprio -j &lt;jobid&gt;</code>: <pre><code>$ sprio -j 1282256\n  JOBID PARTITION   PRIORITY       SITE        AGE  FAIRSHARE    JOBSIZE        QOS\n1282256 general          586          0        129        619        138          0\n</code></pre></p> <p>The job age and size factors are important to avoid the situation where large jobs can get stuck in the queue for a long time because smaller jobs will always fit in everywhere much more easily. The age factor will max out to <code>1.0</code> when 3 days of queue time has been accrued for any job. The job size factor is directly proportional to the number of CPUs requested, regardless of the time limit, and is normalized to the total number of CPUs in the cluster. Therefore the weight of it will always be configured to be equal to the total number of (physical) CPUs available in the cluster.</p>"},{"location":"slurm/accounting/#the-fair-share-factor","title":"The fair-share factor","text":"<p>As the name implies, the fair-share factor is used to ensure that users within each account have their fair share of computing resources made available to them over time. Because the individual research groups have contributed with different amounts of hardware to the cluster, the overall share of computing resources made available to them should match accordingly. Secondly, the resource usage of individual users within each account is important to consider as well, so that users who may recently have vastly overused their shares within each account will not have the highest priority. The goal of the fair-share factor is to balance the usage of all users by adjusting job priorities, so that it's possible for everyone to use their fair share of computing resources over time. The fair-share factor is calculated according to the fair-tree algorithm, which is an integrated part of the SLURM scheduler. It has been configured with a usage decay half-life of 2 weeks, and the usage is completely reset at the first day of each month.</p> <p>To see the current fair-share factor for your user and the amount of shares available for each account, you can run <code>sshare</code>:</p> <pre><code>$ sshare\nAccount                    User  RawShares  NormShares    RawUsage  EffectvUsage  FairShare \n-------------------- ---------- ---------- ----------- ----------- ------------- ---------- \nroot                                          0.000000   699456285      1.000000            \n root                abc@bio.a+          1    0.000548     1136501      0.001625   0.115183 \n ao                                     16    0.008762           0      0.000000            \n jln                                   272    0.148959   148170579      0.211843            \n kln                                    16    0.008762    48720153      0.069656            \n kt                                     48    0.026287    60884827      0.087049            \n ma                                    624    0.341731   185384914      0.265043            \n md                                    259    0.141840    60612473      0.086659            \n ml                                     16    0.008762     2789545      0.003974            \n mms                                    48    0.026287    92148476      0.131738            \n mto                                    16    0.008762           0      0.000000            \n ndj                                    16    0.008762     5181213      0.007408            \n phn                                   381    0.208653    62047356      0.088711            \n pk                                     16    0.008762           1      0.000000            \n rw                                     16    0.008762    18346721      0.026231            \n sss                                    48    0.026287    13392398      0.019147            \n students                               16    0.008762      499066      0.000714            \n ts                                     16    0.008762      142053      0.000203            \n</code></pre> <ul> <li><code>RawShares</code>: the amount of \"shares\" assigned to each account (in our setup simply the number of CPUs each account has contributed with)</li> <li><code>NormShares</code>: the fraction of shares given to each account normalized to the total shares available across all accounts, e.g. a value of 0.33 means an account has been assigned 33% of all the resources available in the cluster.</li> <li><code>RawUsage</code>: usage of all jobs charged to the account or user. The value will decay over time depending on the usage decay half-life configured. The <code>RawUsage</code> for an account is the sum of the <code>RawUsage</code> for each user within the account, thus indicative of which users have contributed the most to the account\u2019s overall score.</li> <li><code>EffectvUsage</code>: <code>RawUsage</code> divided by the total <code>RawUsage</code> for the cluster, hence the column always sums to <code>1.0</code>. <code>EffectvUsage</code> is therefore the percentage of the total cluster usage the account has actually used (in relation to the total usage, NOT the total capacity). In the example above, the <code>ma</code> account has used <code>26.5%</code> of the total cluster usage since the last usage reset.</li> <li><code>FairShare</code>: The fair-share score calculated using the following formula <code>FS = 2^(-EffectvUsage/NormShares)</code>. The <code>FairShare</code> score can be interpreted by the following intervals: <ul> <li>1.0: Unused. The account has not run any jobs since the last usage reset.</li> <li>0.5 - 1.0: Underutilization. The account is underutilizing their granted share. For example a value of 0.75 means the account has underutilized their share 1:2</li> <li>0.5: Average utilization. The account on average is using exactly as much as their granted share.</li> <li>0.0 - 0.5: Over-utilization. The account is overusing their granted share. For example a value of 0.75 means the account has recently overutilized their share 2:1</li> <li>0: No share left. The account is vastly overusing their granted share and users will get the lowest possible priority.</li> </ul> </li> </ul> The fair-share factor and CPU efficiency <p>The value of the fair-share factor is calculated based on CPU usage in units of allocation seconds and not CPU seconds, which is normally the unit used for CPU usage reported by the <code>sreport</code> and <code>sacct</code> commands. Therefore, this also means that the CPU efficiency of past jobs directly impacts how much actual work can be done by the allocated CPUs for each user within each account before their fair share of resources is consumed for the period.</p> <p>For more details about job prioritization see the SLURM documentation and this presentation.</p>"},{"location":"slurm/accounting/#qos-info-and-limitations","title":"QOS info and limitations","text":"<p>See all available QOS and their limitations: <pre><code>$ sacctmgr show qos format=\"name,priority,usagefactor,mintres%20,maxtrespu,maxjobspu\"\n      Name   Priority UsageFactor              MinTRES     MaxTRESPU MaxJobsPU \n---------- ---------- ----------- -------------------- ------------- --------- \n    normal          0    1.000000       cpu=1,mem=512M       cpu=384       500 \n  highprio          1    2.000000       cpu=1,mem=512M       cpu=1024     2000 \n</code></pre></p> <p>See details about account associations, allowed QOS's, and more, for your user: <pre><code># your user\n$ sacctmgr show user withassoc where name=$USER\n      User   Def Acct     Admin    Cluster    Account  Partition     Share   Priority MaxJobs MaxNodes  MaxCPUs MaxSubmit     MaxWall  MaxCPUMins                  QOS   Def QOS\n---------- ---------- --------- ---------- ---------- ---------- --------- ---------- ------- -------- -------- --------- ----------- ----------- -------------------- ---------\nabc@bio.a+       root      None   biocloud       root                    1          1                                                                  highprio,normal    normal\n\n# all users\n$ sacctmgr show user withassoc | less\n</code></pre></p>"},{"location":"slurm/accounting/#undergraduate-students","title":"Undergraduate students","text":"<p>Undergraduate students (up to but NOT including master projects) share resources within the <code>students</code> account and only their combined usage is limited. View current limitations with for example:</p> <pre><code>$ sacctmgr list account students -s format=\"account,priority,MaxCPUs,grpcpus,qos\" | head -n 3\n   Account   Priority  MaxCPUs  GrpCPUs                  QOS \n---------- ---------- -------- -------- -------------------- \n  students                          192               normal\n</code></pre>"},{"location":"slurm/accounting/#job-resource-usage-summary","title":"Job resource usage summary","text":""},{"location":"slurm/accounting/#running-jobs","title":"Running jobs","text":"<p>Use <code>sstat</code> to show the status and live usage accounting information of only running jobs. For batch scripts you need to add <code>.batch</code> to the job ID, for example: <pre><code>$ sstat &lt;job_id&gt;.batch\n</code></pre></p> <p>This will print EVERY metric, so it's nice to select only a few most relevant ones, for example:</p> <pre><code>$ sstat --jobs &lt;job_id&gt;.batch --format=jobid,avecpu,maxrss,ntasks\n</code></pre> Useful format variables Variable Description avecpu Average CPU time of all tasks in job. averss Average resident set size of all tasks. avevmsize Average virtual memory of all tasks in a job. jobid The id of the Job. maxrss Maximum number of bytes read by all tasks in the job. maxvsize Maximum number of bytes written by all tasks in the job. ntasks Number of tasks in a job. <p>For all variables see the SLURM documentation</p>"},{"location":"slurm/accounting/#past-jobs","title":"Past jobs","text":"<p>To view the status of past jobs and their usage accounting information use <code>sacct</code>. <code>sacct</code> will return everything accounted for by default which is very inconvenient to view in a terminal window, so the below command will show the most essential information: <pre><code>$ sacct -o jobid,jobname,start,end,NNodes,NCPUS,ReqMem,CPUTime,AveRSS,MaxRSS --user=$USER --units=G -j 138\nJobID           JobName               Start                 End   NNodes      NCPUS     ReqMem    CPUTime     AveRSS     MaxRSS \n------------ ---------- ------------------- ------------------- -------- ---------- ---------- ---------- ---------- ---------- \n138          interacti+ 2023-11-21T10:43:48 2023-11-21T10:43:59        1         16        20G   00:02:56                       \n138.interac+ interacti+ 2023-11-21T10:43:48 2023-11-21T10:43:59        1         16              00:02:56          0          0 \n138.extern       extern 2023-11-21T10:43:48 2023-11-21T10:43:59        1         16              00:02:56      0.00G      0.00G \n</code></pre></p> <p>There are a huge number of other options to show, see SLURM docs. If you really want to see everything use <code>sacct --long &gt; file.txt</code> and dump it into a file or else it's too much for the terminal.</p>"},{"location":"slurm/accounting/#reservation-usage","title":"Reservation usage","text":"<p>Show reservation usage in CPU hours and percent of the reservation total used <pre><code>$ sreport reservation utilization -t hourper\n--------------------------------------------------------------------------------\nReservation Utilization 2024-11-04T00:00:00 - 2024-11-04T23:59:59\nUsage reported in TRES Hours/Percentage of Total\n--------------------------------------------------------------------------------\n  Cluster      Name               Start                 End      TRES Name                     Allocated                          Idle \n--------- --------- ------------------- ------------------- -------------- ----------------------------- ----------------------------- \n biocloud  amplicon 2024-11-04T08:00:00 2024-11-05T15:18:55            cpu                  1154(19.20%)                  4858(80.80%) \n</code></pre></p>"},{"location":"slurm/accounting/#job-efficiency-summary","title":"Job efficiency summary","text":""},{"location":"slurm/accounting/#individual-jobs","title":"Individual jobs","text":"<p>To view the efficiencies of individual jobs use <code>seff</code>, for example:</p> <pre><code>$ seff 2357\nJob ID: 2357\nCluster: biocloud\nUser/Group: &lt;username&gt;\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 96\nCPU Utilized: 60-11:06:29\nCPU Efficiency: 45.76% of 132-02:48:00 core-walltime\nJob Wall-clock time: 1-09:01:45\nMemory Utilized: 383.42 GB\nMemory Efficiency: 45.11% of 850.00 GB\n</code></pre> <p>This information will also be shown in notification emails when jobs finish.</p>"},{"location":"slurm/accounting/#multiple-jobs","title":"Multiple jobs","text":"<p>Perhaps a more useful way to use <code>sacct</code> is through the reportseff tool (pre-installed), which can be used to calculate the CPU, memory, and time efficiencies of past jobs, so that you can optimize future jobs and ensure resources are utilized to the max (2TM). For example: <pre><code>$ reportseff -u $(whoami) --format partition,jobid,state,jobname,alloccpus,reqmem,elapsed,cputime,CPUEff,MemEff,TimeEff -S r,pd,s --since d=4\n  Partition     JobID        State                       JobName                    AllocCPUS     ReqMem   Elapsed        CPUTime      CPUEff   MemEff   TimeEff \n    shared      306282     COMPLETED                     midasok                        2         8G       00:13:07       00:26:14      4.1%    15.4%     10.9%  \n   general      306290     COMPLETED           smk-map2db-sample=barcode46             16         24G      00:01:38       00:26:08     90.6%    18.7%     2.7%   \n   general      306291     COMPLETED           smk-map2db-sample=barcode47             16         24G      00:02:14       00:35:44     92.8%    18.4%     3.7%   \n   general      306292     COMPLETED           smk-map2db-sample=barcode58             16         24G      00:02:32       00:40:32     80.3%    19.0%     4.2%   \n   general      306293     COMPLETED           smk-map2db-sample=barcode34             16         24G      00:02:16       00:36:16     78.1%    18.7%     3.8%   \n   general      306294     COMPLETED           smk-map2db-sample=barcode22             16         24G      00:02:38       00:42:08     81.1%    19.0%     4.4%   \n   general      306295     COMPLETED           smk-map2db-sample=barcode35             16         24G      00:02:26       00:38:56     79.0%    19.0%     4.1%   \n   general      306296     COMPLETED           smk-map2db-sample=barcode82             16         24G      00:01:39       00:26:24     68.9%    17.8%     2.8%   \n   general      306297     COMPLETED           smk-map2db-sample=barcode70             16         24G      00:02:04       00:33:04     76.0%    19.5%     3.4%   \n   general      306298     COMPLETED           smk-map2db-sample=barcode94             16         24G      00:01:44       00:27:44     70.1%    17.9%     2.9%   \n   general      306331     COMPLETED           smk-map2db-sample=barcode59             16         24G      00:04:00       01:04:00     87.2%    19.6%     6.7%   \n   general      306332     COMPLETED           smk-map2db-sample=barcode83             16         24G      00:02:13       00:35:28     76.3%    19.0%     3.7%   \n   general      306333     COMPLETED           smk-map2db-sample=barcode11             16         24G      00:02:49       00:45:04     81.6%    19.4%     4.7%   \n   general      306334     COMPLETED           smk-map2db-sample=barcode23             16         24G      00:03:33       00:56:48     61.6%    19.0%     5.9%   \n   general      306598     COMPLETED      smk-mapping_overview-sample=barcode46         1         4G       00:00:09       00:00:09     77.8%     0.0%     1.5%   \n   general      306601     COMPLETED      smk-mapping_overview-sample=barcode82         1         4G       00:00:09       00:00:09     77.8%     0.0%     1.5%   \n   general      306625     COMPLETED      smk-mapping_overview-sample=barcode94         1         4G       00:00:07       00:00:07     71.4%     0.0%     1.2%   \n   general      306628     COMPLETED      smk-concatenate_fastq-sample=barcode71        1         1G       00:00:07       00:00:07     71.4%     0.0%     1.2%   \n   general      306629     COMPLETED      smk-concatenate_fastq-sample=barcode70        1         1G       00:00:07       00:00:07     71.4%     0.0%     1.2%   \n   general      306630     COMPLETED      smk-concatenate_fastq-sample=barcode34        1         1G       00:00:07       00:00:07     57.1%     0.0%     1.2%   \n</code></pre></p> <p>In the example above, way too much memory was requested for all the jobs in general, and also the time limits were way too long. The most important is the CPU efficiency, however, which was generally good except for one job, but it was a very small job.</p>"},{"location":"slurm/accounting/#usage-reports","title":"Usage reports","text":"<p><code>sreport</code> can be used to summarize usage in many different ways, below are some examples.</p>"},{"location":"slurm/accounting/#account-usage-by-user","title":"Account usage by user","text":"<pre><code>$ sreport cluster AccountUtilizationByUser format=account%8,login%23,used%10 -t hourper start=\"now-1week\"\n--------------------------------------------------------------------------------\nCluster/Account/User Utilization 2024-03-13T12:00:00 - 2024-03-19T23:59:59 (561600 secs)\nUsage reported in CPU Hours/Percentage of Total\n--------------------------------------------------------------------------------\n Account                   Login               Used \n-------- ----------------------- ------------------ \n    root                             154251(63.71%) \n    root              &lt;username&gt;      37176(15.35%) \n     jln                                3988(1.65%) \n     jln              &lt;username&gt;        2010(0.83%) \n     jln              &lt;username&gt;           0(0.00%) \n     jln              &lt;username&gt;        1978(0.82%) \n     kln                                   7(0.00%) \n     kln              &lt;username&gt;           7(0.00%) \n      ma                               13460(5.56%) \n      ma              &lt;username&gt;        6152(2.54%) \n      ma              &lt;username&gt;        2504(1.03%) \n      ma              &lt;username&gt;        3710(1.53%) \n      ma              &lt;username&gt;         963(0.40%) \n      ma              &lt;username&gt;         131(0.05%) \n      md                              52921(21.86%) \n      md              &lt;username&gt;       18690(7.72%) \n      md              &lt;username&gt;       22443(9.27%) \n      md              &lt;username&gt;       11788(4.87%) \n     mms                               17036(7.04%) \n     mms              &lt;username&gt;         600(0.25%) \n     mms              &lt;username&gt;       16436(6.79%) \n     phn                                6799(2.81%) \n     phn              &lt;username&gt;         114(0.05%) \n     phn              &lt;username&gt;          31(0.01%) \n     phn              &lt;username&gt;        6654(2.75%) \n     phn              &lt;username&gt;           0(0.00%) \n     sss                               22081(9.12%) \n     sss              &lt;username&gt;       22081(9.12%) \nstudents                                 638(0.26%) \nstudents              &lt;username&gt;         638(0.26%) \n</code></pre>"},{"location":"slurm/accounting/#user-usage-by-account","title":"User usage by account","text":"<pre><code>$ sreport cluster UserUtilizationByAccount format=login%23,account%8,used%10 -t hourper start=\"now-1week\"\n--------------------------------------------------------------------------------\nCluster/User/Account Utilization 2024-03-13T12:00:00 - 2024-03-19T23:59:59 (561600 secs)\nUsage reported in CPU Hours/Percentage of Total\n--------------------------------------------------------------------------------\n                  Login  Account               Used \n----------------------- -------- ------------------ \n             &lt;username&gt;     root      37176(15.35%) \n             &lt;username&gt;       md       22443(9.27%) \n             &lt;username&gt;      sss       22081(9.12%) \n             &lt;username&gt;       md       18690(7.72%) \n             &lt;username&gt;      mms       16436(6.79%) \n             &lt;username&gt;       md       11788(4.87%) \n             &lt;username&gt;      phn        6654(2.75%) \n             &lt;username&gt;       ma        6152(2.54%) \n             &lt;username&gt;       ma        3710(1.53%) \n             &lt;username&gt;       ma        2504(1.03%) \n             &lt;username&gt;      jln        2010(0.83%) \n             &lt;username&gt;      jln        1978(0.82%) \n             &lt;username&gt;       ma         963(0.40%) \n             &lt;username&gt; students         638(0.26%) \n             &lt;username&gt;      mms         600(0.25%) \n             &lt;username&gt; compute+         145(0.06%) \n             &lt;username&gt;       ma         131(0.05%) \n             &lt;username&gt;      phn         114(0.05%) \n             &lt;username&gt;      phn          31(0.01%) \n             &lt;username&gt;      kln           7(0.00%) \n             &lt;username&gt;      phn           0(0.00%) \n             &lt;username&gt;      jln           0(0.00%) \n</code></pre>"},{"location":"slurm/accounting/#top-users","title":"Top users","text":"<pre><code>$ sreport user top topcount=20 format=login%21,account%8,used%10 start=\"now-1week\" -t hourper\n--------------------------------------------------------------------------------\nTop 20 Users 2024-03-13T11:00:00 - 2024-03-19T23:59:59 (565200 secs)\nUsage reported in CPU Hours/Percentage of Total\n--------------------------------------------------------------------------------\n                Login  Account               Used \n--------------------- -------- ------------------ \n          &lt;username&gt;      root      37176(15.26%) \n          &lt;username&gt;        md       22698(9.32%) \n          &lt;username&gt;       sss       22082(9.06%) \n          &lt;username&gt;        md       18850(7.74%) \n          &lt;username&gt;       mms       16436(6.75%) \n          &lt;username&gt;        md       12038(4.94%) \n          &lt;username&gt;       phn        6654(2.73%) \n          &lt;username&gt;        ma        6167(2.53%) \n          &lt;username&gt;        ma        3780(1.55%) \n          &lt;username&gt;        ma        2504(1.03%) \n          &lt;username&gt;       jln        2383(0.98%) \n          &lt;username&gt;       jln        1978(0.81%) \n          &lt;username&gt;        ma         963(0.40%) \n          &lt;username&gt;  students         648(0.27%) \n          &lt;username&gt;       mms         600(0.25%) \n          &lt;username&gt;        kt         146(0.06%) \n          &lt;username&gt;        ma         133(0.05%) \n          &lt;username&gt;       phn         114(0.05%) \n          &lt;username&gt;       kln          98(0.04%) \n          &lt;username&gt;       phn          31(0.01%) \n</code></pre>"},{"location":"slurm/efficiency/","title":"Optimizing CPU efficiency","text":""},{"location":"slurm/efficiency/#how-many-resources-should-i-request-for-my-jobs","title":"How many resources should I request for my job(s)?","text":"<p>Exactly how many resources (CPUs, memory, time, GPUs) your job(s) need(s) is something you have to experiment with and learn over time based on past experience. It's important to do a bit of experimentation before submitting large jobs to obtain a qualified guess since the utilization of all the allocated resources across the cluster is ultimately based on people's own assessments alone. Below are some tips regarding CPU and memory, but in the end please always only request what you need, and no more. This is essential to optimize the resource utilization and efficiency of the entire cluster, leaving no CPUs idle when everything is fully allocated.</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When your job completes or fails, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used.</p>"},{"location":"slurm/efficiency/#cpusthreads","title":"CPUs/threads","text":"<p>In general the number of CPUs that you book only affects how long the job will take to finish and how many jobs can run concurrently. The only thing to really consider is how many CPUs you want to use for the particular job out of your max limit (see Usage accounting for how to see the current limits). If you use all CPUs for one job, you can't start more jobs until the first one has finished, the choice is yours. But regardless, it's very important to ensure that your jobs actually fully utilize the allocated number of CPUs, so don't start a job with <code>20</code> allocated CPUs if you only set max threads for a certain tool to <code>10</code>, for example. It also depends very much on the specific software tools you use for the individual steps in a workflow and how they are implemented, so you are not always in full control of the utilization. Furthermore, if you run a workflow with many different steps each using different tools, they will likely not use resources in the same way, and some may not even support multithreading at all (like R, depending on the packages used) and thus only run in a single single-threaded process, for example. In this case it might be a good idea to either split the job into multiple jobs if they run for a long time, or use workflow tools that support cluster execution, for example Snakemake where you can define separate resource requirements for individual steps. This is also the case for memory usage.</p>"},{"location":"slurm/efficiency/#over-subscription","title":"Over-subscription","text":"<p>Sometimes there is just no way around it, and if you don't expect your job(s) to be very efficient, please submit to the overprovisioned <code>shared</code> partition, which is also the default. Over-subscription simply means that SLURM will allocate more CPU's than available on each machine, so that more than one job will run on each CPU, ensuring that each physical CPU is actually utilized 100% and thus more people are happy!</p>"},{"location":"slurm/efficiency/#use-nproc-everywhere","title":"Use <code>nproc</code> everywhere","text":"<p>The number of CPUs is not a hard limit like the physical amount of memory is, on the other hand, and SLURM will never exceed the maximum physical memory of each compute node. Instead jobs are killed if they exceed the allocated amount of memory for the job (only if no other jobs need the memory), or not be allowed to start in the first place. With CPUs the processes you run simply won't be able to detect any more CPUs than those allocated, hence it's handy to just use <code>nproc</code> within scripts to detect the number of available CPUs instead of manually setting a value for each tool. Furthermore, if you request more memory per CPU that the max allowed for the particular partition (refer to hardware partitions), SLURM will automatically allocate more CPU's for the job, and hence, again, it's a good idea to detect the number of CPU's dynamically using <code>nproc</code> everywhere.</p>"},{"location":"slurm/efficiency/#memory","title":"Memory","text":"<p>Requesting a sensible maximum amount of memory is important to avoid crashing jobs. It's generally best to allocate more memory than what you need, so that the job doesn't crash and the spent resources don't go to waste and could have been used for something else anyways. To obtain a qualified guess you can start the job based on an initial expectation, and then set a job time limit of maybe 5-10 minutes just to see if it might crash due to exceeding the allocated memory, and if not you will see the maximum memory usage for the job in the email notification report (or use <code>seff &lt;jobid&gt;</code>). Then adjust accordingly and submit again with a little extra than what was used at maximum. Different steps of a workflow will in many cases, unavoidably, need more memory than others, so again, if they run for a long time, split it into multiple jobs or use Snakemake.</p> <p>Our compute nodes have plenty of memory, but some tools require lots of memory. If you know that your job is going to use a lot of memory (per CPU that is), you should likely submit the job to the <code>high-mem</code> partition. In order to fully utilize each compute node a general rule of thumb is to:</p> Rule of thumb for optimal memory usage <p>Request a maximum of 5GB per CPU (1TB / 192t) if submitting jobs to the <code>general</code> partition. If you need more than that submit to the <code>high-mem</code> partition instead. If you request more memory per CPU than allowed for a particular partition, SLURM will automatically allocate more CPUs to scale accordingly, details here. It is therefore ideal to detect the number of CPUs available dynamically in your workflows using for example <code>nproc</code>.</p> <p>If you know that you are almost going to fully saturate the memory on a compute node (depending on partition), you might as well also request more CPUs up to the total of a single compute node, since your job will likely allocate a full compute node alone, and CPU's can end up idle, while you could have finished the job faster. If needed you can also submit directly to the individual compute nodes specifically using the <code>nodelist</code> option (and potentially also <code>--exclusive</code>), refer to hardware partitions for hostnames and compute node specs.</p> <p>Also keep in mind that the effective amount of memory available to SLURM jobs is less than what the physical machines have available because they are virtual machines running on a hypervisor OS that also needs some memory. A 1 TB machine roughly has 950 GB available and the 2 TB ones have 1.9 TB. See <code>sinfo -N -l</code> for details of each node.</p>"},{"location":"slurm/intro/","title":"Introduction to SLURM","text":"<p>SLURM (Simple Linux Utility for Resource Management) is a highly flexible and powerful job scheduler for managing and scheduling computational workloads on high-performance computing (HPC) clusters. SLURM is designed to efficiently allocate resources and manage job execution on clusters of any size, from a single server to tens of thousands. SLURM manages resources on an HPC cluster by dividing compute nodes into partitions. Users submit jobs with specified resource requirements to these partitions from a login-node, and then the SLURM controller schedules and allocates resources to those jobs based on available resources. SLURM also stores detailed usage information of all jobs in a usage accounting database, which allows enforcement of fair-share policies and priorities for job scheduling for each partition. For an up-to-date list see the hardware overview.</p>"},{"location":"slurm/intro/#biocloud-slurm-cluster-overview","title":"BioCloud SLURM cluster overview","text":"<p>(Note, the exact partitions in the figure may be outdated, but the setup is the same)</p>"},{"location":"slurm/intro/#getting-started","title":"Getting started","text":"<p>Start with obtaining shell access to either of the 3 login nodes <code>bio-ospikachu[01-03].srv.aau.dk</code> as described in Getting access. To start with it's always nice to get an overview of the cluster, it's partitions, and how many resources that are currently allocated. This is achieved with the <code>sinfo</code> command, example output:</p> <pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ngeneral*     up 14-00:00:0      3    mix bio-oscloud[02-04]\ngeneral*     up 14-00:00:0      2   idle bio-oscloud[05-06]\nhigh-mem     up 28-00:00:0      1  boot^ bio-oscloud07\nhigh-mem     up 28-00:00:0      1  alloc bio-oscloud08\ngpu          up 14-00:00:0      1    mix bio-oscloud09\n</code></pre> <p>To get an overview of running jobs use <code>squeue</code>, example output: <pre><code># everything\n$ squeue\n JOBID         NAME       USER       TIME    TIME_LEFT CPU MIN_ME ST PARTITION NODELIST(REASON)\n  2380         dRep user01@bio 9-00:31:24   4-23:28:36  80   300G  R   general bio-oscloud02\n  2389        dramv user02@bio 8-16:14:07   5-07:45:53  16   300G  R   general bio-oscloud02\n  3352       METAGE user03@cs. 1-13:00:45     10:59:15  10   125G  R   general bio-oscloud03\n  3359      ar-gtdb user02@bio 1-00:04:32   5-23:55:28  32   300G  R   general bio-oscloud03\n  3361     bac_gtdb user02@bio 1-00:03:05   5-23:56:55  32   500G  R   general bio-oscloud04\n  3366  interactive user04@bio    2:03:42   2-00:56:18  60   128G  R   general bio-oscloud02\n  3426       blastn user05@bio      41:56   3-23:18:04  96   200G  R   general bio-oscloud03\n  3430  interactive user06@bio       7:23      3:52:37   1     2G  R   general bio-oscloud02\n  3333 as-predictio user07@bio 2-19:42:49   6-04:17:11   5    16G  R       gpu bio-oscloud09\n  3372      checkm2 user02@bio   21:37:50   6-02:22:10 192  1800G  R  high-mem bio-oscloud08\n\n# your own jobs only\n$ squeue --me\n JOBID         NAME       USER       TIME    TIME_LEFT CPU MIN_ME ST PARTITION NODELIST(REASON)\n  3333 as-predictio user07@bio 2-19:42:49   6-04:17:11   5    16G  R       gpu bio-oscloud09\n</code></pre></p> <p>Or get a more detailed overview per compute node of current resource allocations and which jobs are running etc with the wrapper script from slurm_tools <code>pestat</code>: <pre><code>$ pestat\nHostname            Partition     Node Num_CPU  CPUload  Memsize  Freemem  Joblist\n                                 State Use/Tot  (15min)     (MB)     (MB)  JobID User ...\nbio-oscloud02        general*     mix  157 192   78.53*   957078   714399  2380 user01@bio.aau.dk 2389 user02@bio.aau.dk 3366 user04@bio.aau.dk 3430 user06@bio.aau.dk  \nbio-oscloud03        general*     mix  138 192   99.74*   957078   737701  3352 user03@cs.aau.dk 3359 user02@bio.aau.dk 3426 user05@bio.aau.dk  \nbio-oscloud04        general*     mix   32 192    3.86*   957078   712866  3361 user02@bio.aau.dk  \nbio-oscloud05        general*    idle    0 192    0.00    957078  1000920   \nbio-oscloud06        general*    idle    0 192    0.07    957078   968067   \nbio-oscloud07        high-mem    idle    0 240    0.00   1914660  2011251   \nbio-oscloud08        high-mem   alloc  192 192  126.01*  1914680  1843547  3372 user02@bio.aau.dk  \nbio-oscloud09             gpu     mix    5  64    2.36*   214195   206950  3333 user07@bio.aau.dk  \n</code></pre></p> <p>See <code>pestat -h</code> for more options.</p>"},{"location":"slurm/jobcontrol/","title":"Job control and useful commands","text":"<p>Below are some nice to know commands for controlling and checking up on running or queued jobs.</p>"},{"location":"slurm/jobcontrol/#overall-cluster-status","title":"Overall cluster status","text":"<p>This will normally show some colored bars for each partition, which unfortunately doesn't render here. <pre><code>Cluster allocation summary per partition or individual nodes (-n).\n(Numbers are reported in free/allocated/total(OS factor)).\n\nPartition    |                CPUs                 |           Memory (GB)           |       GPUs        |\n========================================================================================================\nshared       | 1436 196                 /1632 (3x) | 2091 268                 /2359  |           \ngeneral      |  395 373                 /768       | 2970 731                 /3701  |           \nhigh-mem     |  233 199                 /432       | 1803 1936                /3739  |           \ngpu          |   24 40                  /64        |   29 180                 /209   |    1 1         /2    \n--------------------------------------------------------------------------------------------------------\nTotal:       | 2088 808                 /2896      | 6894 3115                /10009 |    1 1         /2    \n\nJobs running/pending/total:\n  26 / 1 / 27\n\nUse sinfo or squeue to obtain more details.\n</code></pre></p>"},{"location":"slurm/jobcontrol/#get-job-status-info","title":"Get job status info","text":"<p>Use <code>squeue</code>, for example: <pre><code>$ squeue\nsqueue\n      JOBID             NAME       USER ACCOUNT        TIME   TIME_LEFT CPU MIN_ME ST PRIO  PARTITION NODELIST(REASON)\n    1275175    RStudioServer user01@bio     acc1       0:00  3-00:00:00  32     5G PD    4    general (QOSMaxCpuPerUserLimit)\n    1275180       sshdbridge user02@bio     acc2       7:14     7:52:46   8    40G  R    6    general bio-oscloud03\n    1275170   VirtualDesktop user03@bio     acc2      35:54     5:24:06   2    10G  R    6    general bio-oscloud05\n</code></pre></p> <p>To show only your own jobs use <code>squeue --me</code>. This is used quite often so <code>sq</code> has been made an alias of <code>squeue --me</code>. You can for example also append <code>--partition</code>, <code>--nodelist</code>, <code>--reservation</code>, and more, to only show the queue for those select partitions, nodes, or reservations.</p> <p>You can also get an estimated start time for pending jobs by using <code>squeue --start</code>. Jobs will in most cases start earlier than this time, as the calculation is based on the time limit set for current running jobs, and most finish in time. They can only start later if new jobs with a higher priority get submitted to the queue before they start:</p> <pre><code>$ squeue --start\n    JOBID PARTITION     NAME     USER ST          START_TIME  NODES SCHEDNODES           NODELIST(REASON)\n  1306529   general smk-mapp user01@b PD 2025-01-24T12:25:15      1 bio-oscloud02        (Resources)\n  1306530   general smk-mapp user01@b PD 2025-01-24T12:25:15      1 bio-oscloud04        (Priority)\n  1309386   general cq_EV_me user02@b PD 2025-01-24T12:25:15      1 bio-oscloud05        (Priority)\n  1306531   general smk-mapp user01@b PD 2025-01-24T12:27:02      1 bio-oscloud03        (Priority)\n  1306532   general smk-mapp user01@b PD 2025-01-25T14:46:56      1 (null)               (Priority)\n  1306533   general smk-mapp user01@b PD 2025-01-25T14:46:56      1 (null)               (Priority)\n...\n</code></pre> Job state codes (ST) Status  Code Explaination COMPLETED CD COMPLETING CG FAILED F PENDING PD PREEMPTED PR RUNNING R SUSPENDED S STOPPED ST <p>A complete list can be found in SLURM's documentation</p> Job reason codes (REASON ) Reason Code Explaination Priority One or more higher priority jobs is in queue for running. Your job will eventually run. Dependency This job is waiting for a dependent job to complete and will run afterwards. Resources The job is waiting for resources to become available and will eventually run. InvalidAccount The job\u2019s account is invalid. Cancel the job and rerun with correct account. InvaldQoS The job\u2019s QoS is invalid. Cancel the job and rerun with correct account. QOSGrpCpuLimit All CPUs assigned to your job\u2019s specified QoS are in use; job will run eventually. QOSGrpMaxJobsLimit Maximum number of jobs for your job\u2019s QoS have been met; job will run eventually. QOSGrpNodeLimit All nodes assigned to your job\u2019s specified QoS are in use; job will run eventually. PartitionCpuLimit All CPUs assigned to your job\u2019s specified partition are in use; job will run eventually. PartitionMaxJobsLimit Maximum number of jobs for your job\u2019s partition have been met; job will run eventually. PartitionNodeLimit All nodes assigned to your job\u2019s specified partition are in use; job will run eventually. AssociationCpuLimit All CPUs assigned to your job\u2019s specified association are in use; job will run eventually. AssociationMaxJobsLimit Maximum number of jobs for your job\u2019s association have been met; job will run eventually. AssociationNodeLimit All nodes assigned to your job\u2019s specified association are in use; job will run eventually. <p>A complete list can be found in SLURM's documentation</p> <p>The columns to show can be customized using the <code>--format</code> option, but can also be set with the environment variable <code>SQUEUE_FORMAT</code> to avoid typing it every time. You can always override this to suit your needs in your <code>.bashrc</code> file. The default format is currently:</p> <pre><code>SQUEUE_FORMAT=\"%.12i %.16j %.10u %.10M %.12L %.3C %.6m %.2t %.9P %R\"\n</code></pre> <p>See a full list here.</p>"},{"location":"slurm/jobcontrol/#prevent-pending-job-from-starting","title":"Prevent pending job from starting","text":"<p>Pending jobs can be marked in a \"hold\" state to prevent them from starting <pre><code>scontrol hold &lt;job_id&gt;\n</code></pre></p> <p>To release a queued job from the \u2018hold\u2019 or 'requeued held' states: <pre><code>scontrol release &lt;job_id&gt;\n</code></pre></p> <p>To cancel and rerun (requeue) a particular job: <pre><code>scontrol requeue &lt;job_id&gt;\n</code></pre></p>"},{"location":"slurm/jobcontrol/#cancel-a-job","title":"Cancel a job","text":"<p>With <code>sbatch</code> you won't be able to just hit CTRL+c to stop what's running like you're used to in a terminal. Instead you must use <code>scancel</code>. Get the job ID from <code>squeue --me</code>, then use <code>scancel</code> to cancel a running job, for example: <pre><code>$ scancel &lt;job_id&gt;\n\n# cancel ALL your jobs\n$ scancel --me\n</code></pre></p> <p>If the particular job doesn't stop and doesn't respond, consider using <code>skill</code> instead.</p>"},{"location":"slurm/jobcontrol/#pause-or-resume-a-job","title":"Pause or resume a job","text":"<p>Use <code>scontrol</code> to control your own jobs, for example suspend a running job: <pre><code>$ scontrol suspend &lt;job_id&gt;\n</code></pre></p> <p>Resume again with <pre><code>$ scontrol resume &lt;job_id&gt;\n</code></pre></p>"},{"location":"slurm/jobcontrol/#show-details-about-a-running-or-queued-job","title":"Show details about a running or queued job","text":"<pre><code>scontrol show jobid=&lt;jobid&gt;\n</code></pre> <p>If needed, you can also obtain the batch script used to submit a job: <pre><code>scontrol write batch_script &lt;jobid&gt;\n</code></pre></p>"},{"location":"slurm/jobcontrol/#modifying-job-attributes","title":"Modifying job attributes","text":"<p>Only a few job attributes can be changed after a job is submitted and NOT running yet. These attributes include:</p> <ul> <li>time limit</li> <li>job name</li> <li>job dependency</li> <li>partition or QOS</li> <li>nice value</li> </ul> <p>For example: <pre><code>$ scontrol update JobId=&lt;jobid&gt; timelimit=&lt;new timelimit&gt;\n$ scontrol update JobId=&lt;jobid&gt; partition=high-mem\n</code></pre></p> <p>If the job is already running, adjusting the time limit must be done by an administrator.</p>"},{"location":"slurm/jobsubmission/","title":"Job submission","text":"<p>Once you are logged in to one of the login nodes through SSH, there are several ways to request resources and run jobs at different complexity levels through SLURM, but here are the most essential ways for interactive (foreground) and non-interactive (background) use. Generally you need to be acquainted with 3 SLURM job submission commands depending on your needs. These are <code>srun</code>, <code>salloc</code>, and <code>sbatch</code>. They all share the exact same options to define trackable resource constraints (\"TRES\" in SLURM parlor, fx number of CPUs, memory, GPU, etc), time limits, email for job status notifications, and many other things, but are made for different use-cases, which will be described below.</p> <p>It's important to submit your job to the most appropriate hardware partition to ensure an efficient cluster for everyone, so after reading this page please also read the next page carefully.</p>"},{"location":"slurm/jobsubmission/#interactive-jobs","title":"Interactive jobs","text":"<p>An interactive shell is useful for testing and development purposes where you need resources only for a short time, or to experiment with scripts and workflows on minimal example data before submitting larger jobs using <code>sbatch</code> that will run for much longer in the background instead.</p> Important <p>When using an interactive shell it's important to keep in mind that the allocated resources remain reserved only for you until you <code>exit</code> the shell session. So don't leave it hanging idle for too long if you know you are not going to actively use it, otherwise other users might have needed the resources in the meantime. For the same reasons, it's not allowed to use <code>salloc</code> or <code>srun</code> within an emulated terminal with <code>screen</code> or <code>tmux</code>, because resources will remain reserved even though nothing is running after commands/scripts have finished. It's much better to use <code>sbatch</code> instead. As a last resort if you really insist on an interactive session you can append for example <code>; exit</code> to the last command you execute to ensure that the job allocation is automatically terminated when the command exits (regardless of exit status). Or just use <code>sbatch</code>!! :)</p>"},{"location":"slurm/jobsubmission/#using-the-salloc-command","title":"Using the <code>salloc</code> command","text":"<p>To immediately request and allocate resources (once available) and start an interactive shell session directly on the allocated compute node(s) through SLURM, just type <code>salloc</code>:</p> <pre><code>$ salloc\n</code></pre> <p>Here SLURM will find a compute node with the default amount of resources available (which is 1CPU, 512MB memory, and a 1-hour time limit at the time of writing) and start the session on the allocated compute node(s) within the requested resource constraints. If you need more resources you need to explicitly ask for it, for example:</p> <pre><code>$ salloc --cpus-per-task 2 --mem 4G --time 0-3:00:00\n</code></pre> <p>Resources will then remain allocated until the shell is exited with <code>CTRL+d</code>, typing <code>exit</code>, or closing the window. If it takes more than a few seconds to allocate resources, your job might be queued due to a variety of reasons. If so check the <code>REASON</code> codes for the job with <code>squeue</code> from another session.</p>"},{"location":"slurm/jobsubmission/#using-the-srun-command","title":"Using the <code>srun</code> command","text":"<p>If you just need to run a single command/script in the foreground it's better to use <code>srun</code>, which will run things directly on a compute node instead of first starting an interactive shell. As opposed to <code>salloc</code> the job is terminated immediately once the command/script finishes. Any required software modules or conda environments must be loaded first before issuing the command, for example:</p> <pre><code>$ module load minimap2\n$ srun --cpus-per-task 8 --mem 16G --time 1-00:00:00 minimap2 &lt;options&gt;\n</code></pre> <p>The terminal will be blocked for the entire duration, hence for larger jobs it's ideal to submit a job through <code>sbatch</code> instead, which will run in the background.</p> <p><code>srun</code> is also used to run multiple tasks/steps (parallel processes) within an already obtained resource allocation (job), see example below. SLURM tasks can then span multiple compute nodes at once to distribute highly parallel work at any scale.</p> Checking up on running jobs using <code>srun</code> <p>As the <code>srun</code> command can be used to run commands within any job allocation your user has been granted, it comes in handy to inspect already running jobs. This is possible by obtaining an interactive shell within the job allocation using <code>srun --jobid &lt;jobid&gt; --pty /bin/bash</code>, and then run for example <code>htop</code> or <code>top</code> (or <code>nvidia-smi</code> for GPUs) to inspect CPU and memory usage. Then verify that things are running as expected by ensuring that threads are fully utilizing all or most of the CPUs allocated, if not you should cancel the job and adjust the submission script or commands accordingly. Note that only a single interactive terminal can be active within the same job allocation at any one time.</p>"},{"location":"slurm/jobsubmission/#graphical-apps-gui","title":"Graphical apps (GUI)","text":"<p>In order to run graphical programs simply append the <code>--x11</code> option to <code>salloc</code> or <code>srun</code> and run the program. The graphical app will then show up in a window on your own computer, while running inside a SLURM job on the cluster: <pre><code>$ srun --cpus-per-task 2 --mem 4G --time 0-3:00:00 --x11 /path/to/gui/app\n</code></pre></p> <p>It's important to mention that in order for this to work properly, you must first ensure that you have connected to the particular login node using either the <code>ssh -X</code> option or that you have set the <code>ForwardX11 yes</code> option in your SSH config file, see example here.</p> <p>You can also just use the interactive web portal to start a virtual desktop to run any graphical software, which is especially handy if it needs to run for a long time, because you can log off while it's running and come back later to check the status.</p> Connectivity and interactive jobs <p>Keep in mind that with interactive jobs briefly losing connection to the login-node can result in the job being killed. This is to avoid that resources would otherwise remain blocked due to unresponsive shell sessions. If you still see the job in the <code>squeue</code> overview, however, use <code>sattach</code> to reattach to a running interactive job, just remember to append <code>.interactive</code> to the job ID, fx <code>38.interactive</code>.</p>"},{"location":"slurm/jobsubmission/#non-interactive-jobs","title":"Non-interactive jobs","text":"<p>The most convenient and highly recommended way to run most things is by submitting jobs to the job queue for execution in the background in the form of SLURM batch scripts through the <code>sbatch</code> command. Resource requirements are instead defined by <code>#SBATCH</code> comment-style directives at the top of a shell script, and the script is then submitted to SLURM using a simple <code>sbatch script.sh</code> command. This is ideal for submitting large jobs that will run for many hours or days, but of course also for testing/development work. A SLURM batch script should always contain (in order):</p> <ul> <li>Any number of <code>#SBATCH</code> lines with options defining resource constraints and other options for the subsequent SLURM task(s) to be run.</li> <li>A list of commands to load required software modules or conda environments that are required for all tasks. This can also be done separately within any external scripts being run.</li> <li>The main body of the script/workflow, or call to an external script or program to run within the resource allocation (=job ID)</li> </ul> <p>Submit the batch script to the SLURM job queue using <code>sbatch script.sh</code>, and it will then start once the requested amount of resources are available (also taking into account your past usage and priorities of other jobs etc, all 3 job submission commands do that). If you set the <code>--mail-user</code> and <code>--mail-type</code> arguments you should get a notification email once the job starts and finishes with additional details like how many resources you have actually used compared to what you have requested. This is essential information for future jobs to avoid overbooking and maximize resource utilization of the cluster.</p> <p>You can also simply add <code>#SBATCH</code> lines to any shell script you already have, and also run the script with arguments, so for example instead of <code>bash script.sh -i input -o output ...</code> you can simply run <code>sbatch script.sh -i input -o output ...</code>.</p> Non-interactive job output (<code>stdout</code>/<code>stderr</code> streams) <p>As the job is handled by SLURM in the background by the SLURM daemons on the individual compute nodes you won't see any output to the terminal. It will instead be written to the file(s) defined by <code>--output</code> and/or <code>--error</code>. To follow along in real time use for example <code>tail -f job_123.out</code>.</p>"},{"location":"slurm/jobsubmission/#single-node-single-task-example","title":"Single-node, single-task example","text":"<p>A simple example SLURM <code>sbatch</code> script for a single task could look like this:</p> <pre><code>#!/usr/bin/bash -l\n#SBATCH --job-name=minimap2test\n#SBATCH --output=job_%j_%x.out\n#SBATCH --partition=shared\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=2-00:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# Exit script on the first error\nset -euo pipefail\n\n# load conda environment or software modules\nconda activate minimap2\n\n# Get number of CPUs from the SLURM allocation (SLURM will sometimes \n# give you more than what you have requested to optimize efficiency).\n# This only works with single-node jobs\nmax_threads=\"$(nproc)\"\n\n# run one or more commands as part a full pipeline script or call scripts from elsewhere\nminimap2 -t \"$max_threads\" database.fastq input.fastq &gt; out.file\n</code></pre>"},{"location":"slurm/jobsubmission/#multi-node-multi-task-example","title":"Multi-node, multi-task example","text":"<p>An example SLURM <code>sbatch</code> script for parallel (independent) execution across multiple nodes could look like this:</p> <pre><code>#!/usr/bin/bash -l\n#SBATCH --job-name=minimap2test\n#SBATCH --output=job_%j_%x.out\n#SBATCH --nodes=5\n#SBATCH --ntasks=5\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=shared\n#SBATCH --cpus-per-task=60\n#SBATCH --mem-per-cpu=3G\n#SBATCH --time=2-00:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# Exit script on the first error\nset -euo pipefail\n\n# load conda environment or software modules\nconda activate minimap2\n\n# Must use srun when doing distributed work across multiple nodes\nsrun --ntasks 1 minimap2 -t 60 database.fastq input1.fastq &gt; out.file1\nsrun --ntasks 1 minimap2 -t 60 database.fastq input2.fastq &gt; out.file2\nsrun --ntasks 1 minimap2 -t 60 database.fastq input3.fastq &gt; out.file3\nsrun --ntasks 1 minimap2 -t 60 database.fastq input4.fastq &gt; out.file4\nsrun --ntasks 1 minimap2 -t 60 database.fastq input5.fastq &gt; out.file5\n</code></pre> <p>For more examples of parallel jobs and array jobs, for now see for example this page.</p> Important <p>The <code>bash -l</code> in the top \"shebang\" line is required for the compute nodes to be able to load software modules and conda environments correctly.</p> Jobs that span multiple compute nodes <p>If needed the BioCloud is properly set up with the <code>OpenMPI</code> and <code>PMIx</code> message interfaces for distributed work across multiple compute nodes, but it requires you to tailor your scripts and commands specifically for distributed work and is a topic for another time. You can run \"brute-force parallel\" jobs, however, using for example GNU parallel and distribute them across nodes, but this is only for experienced users and they must figure that out for themselves for now.</p>"},{"location":"slurm/jobsubmission/#requesting-one-or-more-gpus","title":"Requesting one or more GPUs","text":"<p>If you need to use one or more GPUs you need to specify <code>--partition=gpu</code> and set <code>--gres=gpu:x</code>, where <code>x</code> refers to the number of GPUs you need. Please don't do CPU work on the <code>gpu</code> partition unless you also need a GPU. It's also worth considering using <code>--cpus-per-gpu</code> and <code>--mem-per-gpu</code>. Additional details here.</p>"},{"location":"slurm/jobsubmission/#most-essential-options","title":"Most essential options","text":"<p>There are plenty of options with the SLURM job submission commands, but below are the most important ones for our current setup and common use-cases. If you need anything else you can start with the SLURM cheatsheet, or else refer to the SLURM documentation for the individual commands <code>srun</code>, <code>salloc</code>, and <code>sbatch</code>.</p> Option Default value(s) Description <code>--job-name</code> The name of the script A user-defined name for the job or task. This name helps identify the job in logs and accounting records. <code>--begin</code> Now Specifies a start time for the job to begin execution. Jobs won't start before this time. Details here. <code>--output</code>, <code>--error</code> <code>slurm-&lt;jobid&gt;.out</code> Redirect the job's standard output/error (<code>stdout</code>/<code>stderr</code>) to a file, ideally on network storage. All directories in the path must exist before the job can start. By default <code>stderr</code> and <code>stdout</code> are merged into a file <code>slurm-%j.out</code> in the current workdir, where <code>%j</code> is the job allocation number. See filename patterns here. <code>--ntasks-per-node</code> <code>1</code> Specifies the number of tasks to be launched per allocated compute node. <code>--ntasks</code> <code>1</code> Indicates the total number of tasks or processes that the job should execute. <code>--cpus-per-task</code> <code>1</code> Sets the number of CPU cores allocated per task. Required for parallel and multithreaded applications. <code>--mem</code>, <code>--mem-per-cpu</code>, or <code>--mem-per-gpu</code> <code>512MB</code> (per node) Specifies the memory limit per node, or per allocated CPU/GPU. These are mutually exclusive. <code>--nodes</code> <code>1</code> Indicates the total number of compute nodes to be allocated for the job. <code>--nodelist</code> Specifies a comma-separated list of specific compute nodes to be allocated for the job. <code>--exclusive</code> Flag. If set will request exclusive access to a full compute node, meaning no other jobs will be allowed to run on the node. In this case you might as well also use all available memory by setting <code>--mem=0</code>, unless there are suspended jobs on the particular node. Details here. <code>--gres</code> List of \"generic consumable resources\" to use, for example a GPU. Details here. <code>--partition</code> <code>general</code> The SLURM partition to which the job is submitted. Default is to use the <code>shared</code> partition. <code>--reservation</code> Allocate resources for the job from the named reservation(s). Can be a comma-separated list of more than one. Details here <code>--chdir</code> Set the working directory of the batch script before it's executed. Setting this using environment variables is not supported. <code>--time</code> <code>0-01:00:00</code> Defines the maximum time limit for job execution before it will be killed automatically. Format <code>DD-HH:MM:SS</code>. Maximum allowed value is that of the partition used. Details here <code>--mail-type</code> <code>NONE</code> Configures email notifications for certain job events. One or more comma-separated values of: <code>NONE</code>, <code>ALL</code>, <code>BEGIN</code>, <code>END</code>, <code>FAIL</code>, <code>REQUEUE</code>, <code>ARRAY_TASKS</code>. Details here <code>--mail-user</code> Local user Specifies the email address where job notifications are sent. <code>--x11</code> <code>all</code> Enable forwarding of graphical applications from the job to your computer. It's required that you have either connected using the <code>ssh -X</code> option or you have set the <code>ForwardX11 yes</code> option in your SSH config file. For <code>salloc</code> or <code>srun</code> only. Details here. <p>Most options are self-explanatory. But for our setup and common use-cases you almost always want to set <code>--nodes</code> to 1, meaning your job will only run on a single compute node at a time. For multithreaded applications (most are nowadays) you mostly only need to set <code>ntasks</code> to <code>1</code> because threads are spawned from a single process (=task in SLURM parlor), and thus increase <code>--cpus-per-task</code> instead.</p>"},{"location":"slurm/multistep_jobs/","title":"Multi-step jobs","text":"<p>Many steps in a complex workflow will only run on a single thread regardless of whether you've asked for more. This leads to a waste of resources. You can submit separate jobs by writing down the commands in separate shell scripts, then submit them as individual jobs using sbatch with different resource requirements:</p> <p><code>launchscript.sh</code> <pre><code>#!/bin/bash\n\nset -euo pipefail\n\n# Submit the first job step and capture its job ID\nstep1_jobid=$(sbatch step1_script.sh | awk '{print $4}')\n\n# Submit the second job, ensuring it runs only after the first job completes successfully\nstep2_jobid=$(sbatch --dependency=afterok:$step1_jobid step2_script.sh | awk '{print $4}')\n\n# Submit the third/last job, ensuring it runs only after the second job completes successfully\nsbatch --dependency=afterok:$step2_jobid step3_script.sh\n</code></pre></p> <p>Types of Dependencies:  - <code>afterok</code>: The dependent job runs if the first job completes successfully (exit code 0).  - <code>afternotok</code>: The dependent job runs if the first job fails (non-zero exit code).  - <code>afterany</code>: The dependent job runs after the first job completes, regardless of success or failure.  - <code>after:&lt;job_id&gt;</code>: The dependent job starts when the first job begins execution.</p> <p>In this case, using <code>--dependency=afterok</code> ensures that the second job will only start if the first job finishes without errors.</p> <p>Submit using bash not sbatch.</p> <p>This can be repeated as many times as necessary. Any arguments can be passed on to the shell scripts in the exact same way as when invoking them using <code>bash script -i \"some option\" -o \"some other option\"</code> as usual.</p>"},{"location":"slurm/other/","title":"Other commands / FAQ","text":"<p>Below are some nice to know commands with example output and some common problems. This will continously be populated as people ask for certain things. Your question here!</p>"},{"location":"slurm/other/#im-not-allowed-to-submit-jobs","title":"I'm not allowed to submit jobs","text":"<pre><code>salloc: error: Job submit/allocate failed: Invalid account or account/partition combination specified\n</code></pre> <p>This error means you have not been associated with any usage account yet, so you must contact an administrator to add your user to the correct account.</p>"},{"location":"slurm/other/#my-job-is-pending-with-a-requeued-held-status","title":"My job is pending with a \"requeued held\" status","text":"<p>This means something went wrong when the job started on a compute node, so the job went back into the queue to avoid draining the node. It will stay in this state forever until manually restarted. Try running a <code>scontrol release &lt;job_id&gt;</code> and if that doesn't work contact an administrator.</p>"},{"location":"slurm/other/#show-busyfree-cores-for-the-entire-cluster","title":"Show busy/free cores for the entire cluster","text":"<p>Example output (A=allocated, I=idle, O=other, T=total): <pre><code>$ sinfo -o \"%C\"\nCPUS(A/I/O/T)\n620/596/240/1456\n</code></pre></p>"},{"location":"slurm/other/#show-current-reservations","title":"Show current reservations","text":"<p>Reservations will also be used for scheduled maintenance, so that SLURM simply won't allow any jobs to start if they have a <code>time</code> limit set that spans into the reservation. <pre><code>$ sinfo -T\nRESV_NAME       STATE           START_TIME             END_TIME     DURATION  NODELIST\nmaintenance  INACTIVE  2023-12-18T23:00:00  2023-12-20T01:00:00   1-02:00:00  bio-oscloud[02-09]\n</code></pre></p> <p>For more details about one or all reservations use <pre><code>$ scontrol show reservations\nReservationName=amplicon StartTime=2024-11-04T08:00:00 EndTime=2024-11-18T08:00:00 Duration=14-00:00:00\n   Nodes=bio-oscloud03 NodeCnt=1 CoreCnt=192 Features=(null) PartitionName=general Flags=\n     NodeName=bio-oscloud03 CoreIDs=0-191\n   TRES=cpu=192\n   Users=abc@bio.aau.dk Groups=(null) Accounts=(null) Licenses=(null) State=ACTIVE BurstBuffer=(null) Watts=n/a\n   MaxStartDelay=(null)\n</code></pre></p>"},{"location":"slurm/other/#show-details-about-the-whole-cluster-configuration","title":"Show details about the whole cluster configuration","text":"<pre><code>$ scontrol show config\n</code></pre>"},{"location":"slurm/other/#slurm-environment-variables","title":"SLURM environment variables","text":"<p>SLURM jobs will have a variety of environment variables set within job allocations, which might become handy programmatically. Below is an overview of relevant ones. For all of them refer to the SLURM documentation for input environment variables and output environment variables. Some may not be present for your particular job, so to list only those currently available within a job run for example <code>env | grep -iE 'SLURM|SBATCH'</code>.</p> Variable(s) Description <code>SLURM_ARRAY_TASK_COUNT</code> Total number of tasks in a job array <code>SLURM_ARRAY_TASK_ID</code> Job array ID (index) number <code>SLURM_ARRAY_TASK_MAX</code> Job array's maximum ID (index) number <code>SLURM_ARRAY_TASK_MIN</code> Job array's minimum ID (index) number <code>SLURM_ARRAY_TASK_STEP</code> Job array's index step size <code>SLURM_ARRAY_JOB_ID</code> Job array's master job ID number <code>SLURM_CLUSTER_NAME</code> Name of the cluster on which the job is executing <code>SLURM_CPUS_ON_NODE</code> Number of CPUS on the allocated node <code>SLURM_CPUS_PER_TASK</code> Number of cpus requested per task. Only set if the --cpus-per-task option is specified. <code>SLURM_JOB_ACCOUNT</code> Account name associated of the job allocation <code>SLURM_JOBID</code>, <code>SLURM_JOB_ID</code> The ID of the job allocation <code>SLURM_JOB_CPUS_PER_NODE</code> Count of processors available to the job on this <code>SLURM_JOB_DEPENDENCY</code> Set to value of the --dependency option <code>SLURM_JOB_NAME</code> Name of the job <code>SLURM_NODELIST</code>, <code>SLURM_JOB_NODELIST</code> List of nodes allocated to the job <code>SLURM_NNODES</code>, <code>SLURM_JOB_NUM_NODES</code> Total number of different nodes in the job's resource allocation <code>SLURM_MEM_PER_NODE</code> Takes the value of --mem if this option was specified. <code>SLURM_MEM_PER_CPU</code> Takes the value of --mem-per-cpu if this option was specified. <code>SLURM_NTASKS</code>, <code>SLURM_NPROCS</code> Same as -n or --ntasks if either of these options was specified. <code>SLURM_NTASKS_PER_NODE</code> Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. <code>SLURM_NTASKS_PER_SOCKET</code> Number of tasks requested per socket. Only set if the --ntasks-per-socket option is specified. <code>SLURM_SUBMIT_DIR</code> The directory from which sbatch was invoked <code>SLURM_SUBMIT_HOST</code> The hostname of the computer from which sbatch was invoked <code>SLURM_TASK_PID</code> The process ID of the task being started <code>SLURMD_NODENAME</code> Name of the node running the job script <code>SLURM_JOB_GPUS</code> GPU IDs allocated to the job (if any). <p><code>sbcast</code>?</p>"},{"location":"slurm/partitions/","title":"Hardware partitions","text":"<p>Before submitting a job you must carefully choose the correct hardware partition for it. To ensure hardware utilization is maximized the compute nodes are divided into separate partitions depending on their specs and for different use-cases. When submitting a job and choosing a hardware partition for it you should always consider:</p> <ul> <li>The required memory per CPU (e.g. 10 CPUs and 20GB memory is 2GB per CPU). Only this ratio matters, the total memory requirement does NOT matter!</li> <li>The expected CPU efficiency (i.e. do you expect to keep all allocated CPUs busy at 100% at all times?)</li> <li>Whether you need to read/write lots of temporary files (thousands) and benefit from extra local scratch space (more details here)</li> <li>Whether you need to use special hardware like a GPU</li> </ul> <p>If in doubt just use the <code>shared</code> for most things. The overall efficiency of the entire cluster depends entirely on how users submit jobs, and whether they actually use what they request. So please take great care in this regard - it impacts the productivity of all your collegues! The below flow diagram may help choosing the right partition for your needs.</p>"},{"location":"slurm/partitions/#choosing-the-correct-partition","title":"Choosing the correct partition","text":"<p>Always detect available CPUs dynamically in scripts and workflows, never hard-code it!</p> <p>It's very important to note that all partitions have a max memory per CPU configured, which may result in the scheduler allocating more CPU's for the job than requested until this ratio is satisfied. This is to ensure that no CPU's end up idle when a compute node is fully allocated in terms of memory, when it could have been at work finishing jobs faster instead. Therefore NEVER hardcode the number of threads/cores to use for individual software tools, instead make it a habit to detect it dynamically using fx <code>$(nproc)</code> in scripts and workflows.</p>"},{"location":"slurm/partitions/#partitions","title":"Partitions","text":""},{"location":"slurm/partitions/#the-shared-partition","title":"The <code>shared</code> partition","text":"<p>The <code>shared</code> partition is the shared partition and is overprovisoned with a factor of 3, meaning each CPU can run up to 3 jobs at once. This is ideal for low to medium (&lt;75%) efficiency, low to medium memory, and interactive jobs, as well as I/O jobs, that don't fully utilize the allocated CPU's 100% at all times. It's highly recommended to use this partition for most CPU intensive jobs unless you need lots of memory, or have taken extra care optimizing the CPU efficiency of the job(s), in which case you can use the <code>general</code> or <code>high-mem</code> partitions instead and finish the jobs faster. Due to over-subscription it's actually possible to allocate a total of 1056 CPUs for jobs on this partition, however with less memory. The job time limit for this partition is 7 days.</p> <p>Max memory per CPU: 1.5GB</p> Hostname vCPUs Memory Scratch space <code>bio-oscloud01</code> 96 0.5 TB - <code>axomamma</code> 256 1 TB 3.5 TB"},{"location":"slurm/partitions/#the-general-partition","title":"The <code>general</code> partition","text":"<p>The <code>general</code> partition is for high efficiency jobs only, since the CPU's are not shared among multiple jobs, but dedicated to each individual job. Therefore you must ensure that they are also fully utilized at all times, preferably 75-100%, otherwise please use the <code>shared</code> partition instead if the memory (per CPU) is sufficient. The job time limit for this partition is 14 days.</p> <p>Max memory per CPU: 5GB</p> Hostname vCPUs Memory Scratch space <code>bio-oscloud[02-05]</code> 192 1 TB - <code>bio-oscloud06</code> 192 1 TB 18TB"},{"location":"slurm/partitions/#the-high-mem-partition","title":"The <code>high-mem</code> partition","text":"<p>The <code>high-mem</code> partition is only for high efficiency jobs that also require lots of memory. Please do not submit anything here that doesn't require at least 5GB per CPU, otherwise please use the <code>general</code> partition. Like the <code>general</code> partition, the CPU's are dedicated to each individual job. Therefore you must ensure that they are also fully utilized at all times, preferably 75-100%, otherwise please use the <code>shared</code> partition instead if the memory (per CPU) is sufficient. The job time limit for this partition is 28 days.</p> <p>Max memory per CPU: 10GB</p> Hostname vCPUs Memory Scratch space <code>bio-oscloud07</code> 240 2 TB - <code>bio-oscloud08</code> 192 2 TB -"},{"location":"slurm/partitions/#the-gpu-partition","title":"The <code>gpu</code> partition","text":"<p>This partition is ONLY for jobs that require a GPU. Please do not submit jobs to this partition if you don't need a GPU. There is otherwise no max memory per CPU or over-subscription configured for this partition. The job time limit for this partition is 14 days.</p> Hostname vCPUs Memory Scratch space GPU <code>bio-oscloud09</code> 64 256 GB 3 TB 2x nvidia A10"},{"location":"slurm/tldr/","title":"TLDR","text":"<p>Here's a quick \"too long, didn't read\" guide for those who are too scared to read all the details and just want to get started quickly. There is nothing here you don't need to know!</p>"},{"location":"slurm/tldr/#how-to-submit-a-slurm-job","title":"How to submit a SLURM job","text":"<p>Start with logging in to one of the login nodes through SSH.</p>"},{"location":"slurm/tldr/#interactive","title":"Interactive","text":"<p>If you just need an interactive shell to experiment with some quick commands use <code>salloc</code>, for example: <pre><code>salloc --cpus-per-task 2 --mem 4G --time 0-3:00:00\n</code></pre></p> <p>This will start a shell on a compute node within a SLURM allocation (here 2 CPUs, 4GB memory for 3 hours), so that you don't run anything on the login node itself.</p> <p>Important</p> <p>Do NOT run anything on the login nodes except SLURM commands to submit jobs! You can edit code, browse/move files around, install software, etc, but nothing else. Everything else should be run on a dedicated compute node through SLURM allocations using <code>sbatch</code> or <code>salloc</code>. If you run things on the login nodes, people can be blocked from doing anything at all if the login nodes crash, because they are (deliberately) quite small.</p>"},{"location":"slurm/tldr/#non-interactive","title":"Non-interactive","text":"<p>For larger jobs that will run for several hours or days, you need to submit SLURM batch jobs using <code>sbatch</code>:</p> <ul> <li>Write a shell script (using <code>nano</code> or whatever editor you want) named for example <code>submit.sh</code> which contains the command(s) you need to run, and let SLURM know how many resources your job needs by filling in the <code>#SBATCH</code> comments (more options here) at the top, for example:</li> </ul> <p><pre><code>#!/usr/bin/bash -l\n#SBATCH --job-name=minimap2test\n#SBATCH --output=job_%j_%x.out\n#SBATCH --partition=shared\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=2-00:00:00\n#SBATCH --mail-type=START,END,FAIL\n#SBATCH --mail-user=abc@bio.aau.dk\n\n# Exit on first error and if any variables are unset\nset -eu\n\n# load software modules or environments\nmodule load minimap2\n\n# Get number of CPUs from the SLURM allocation (SLURM will sometimes \n# give you more than what you have requested to optimize efficiency).\n# This only works with single-node jobs\nmax_threads=\"$(nproc)\"\n\n# run one or more commands as part a full pipeline script or call scripts from elsewhere\nminimap2 -t \"$max_threads\" database.fastq input.fastq &gt; out.file\n</code></pre> This will request 10 CPUs and 10GB memory for a maximum of 2 days on one of the compute nodes within the <code>shared</code> compute node partition (see hardware overview). If you need to use a GPU details are here.</p> <ul> <li>Submit the job to the queue by typing the command <code>sbatch submit.sh</code></li> <li>Check the job status using <code>squeue --me</code> (or the convenient shorter alias <code>sq</code>). The job will start once a compute node has enough available resources</li> <li>Once the job starts, you can follow the output from the command(s) by inspecting the log file using for example <code>tail -F job_xxx.out</code> (the job will run in the current working directory if not configured otherwise)</li> <li>If needed cancel the job using <code>scancel &lt;jobid&gt;</code></li> </ul>"},{"location":"slurm/tldr/#optimize-efficiency-for-next-time","title":"Optimize efficiency for next time","text":"<p>The job allocation is entirely yours, which means you cannot affect other people's jobs in any way, neither can they affect yours. But this also means that you don't want to waste resources which other users could have used instead, especially CPUs (=time). How many resources your job needs is trial and error, but ideally what you request should be utilized to the greatest extend possible. Therefore:</p> <p>Always inspect and optimize efficiency for next time!</p> <p>When the job completes or fails, !!!ALWAYS!!! inspect the CPU and memory usage of the job in either the notification email received or using these commands and adjust the next job accordingly! This is essential to avoid wasting resources which other people could have used.</p>"},{"location":"slurm/tldr/#choosing-the-right-compute-node-partition","title":"Choosing the right compute node partition","text":"<p>If the CPU and memory efficiency/usage of a job was &gt;75%, you are allowed to submit to other partitions (<code>general</code>, or <code>high-mem</code> if you need lots of memory, see hardware overview) than the <code>shared</code> partition, which can potentially get things done quicker - a win for everyone. But if not, you will waste resources which could have been used by others. The <code>shared</code> partition has less memory available per CPU, however the physical CPUs are shared across jobs to help keep them as busy as possible at all times. The allocated amount of memory will always be yours and yours alone, on the other hand. Many processes will not use all CPUs available for the full duration. This depends heavily on the particular software/tools in use and how they are implemented, as well as the actual input data. Sometimes there is just nothing you can do, but often there is. More details here. </p>"},{"location":"software/conda/","title":"Conda environments","text":"<p>Conda is an open-source package management system and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies, where a sophisticated dependency solver allows installing multiple tools into the same environment at once without introducing conflicts between required versions of the individual dependencies. This is ideal for scientific projects where reproducibility is key - you can simply create a separate conda environment for each individual project.</p> <p>Conda was initially created for Python packages but it can package and distribute software for any language. Conda also doesn't require elevated privileges allowing users to install anything with ease. Most tools are already available in the default Anaconda repository, but other community-driven channels like bioconda allow installing practically anything. In comparison to containers, Conda is a dependency manager at the Python package level, while containers also manage operating system dependencies at the base operating system level, hence containers and conda environments are often used together to ensure complete reproducibility and portability, see for example the biocontainers.pro project.</p> <p>Cheatsheet here</p>"},{"location":"software/conda/#creating-an-environment","title":"Creating an environment","text":"<p>To install software through conda, it must always be done in an environment. Conda itself is already installed and configured on BioCloud, so you don't need to install it first. To create an environment and install some software in it, run for example:</p> <pre><code># create+activate+install\nconda env create -n myproject\nconda activate myproject\nconda install -c bioconda somepkg1=1.0 somepkg2=2.0\n\n# or in one command\nconda create -n myproject -c bioconda somepkg1 somepkg2\n</code></pre> <p>Make sure to add the required conda channels using <code>-c &lt;channel&gt;</code> from which to install the software. Usually the <code>bioconda</code> and <code>conda-forge</code> channels are all you need.</p> <p>The best practice is to always note down all packages including versions used in projects before you forget things to ensure reproducibility. You can always export an activated environment created previously and dump the exact versions used into a YAML file with <code>conda env export &gt; requirements.yml</code>. The file could for example look like this:</p> <p>requirements.yml <pre><code>name: myproject\nchannels:\n  - bioconda\ndependencies:\n - minimap2=2.26\n - samtools=1.18\n</code></pre></p> <p>To create an environment from the file in the future simply run <code>conda env create -f requirements.yml</code>.</p> Note <p>When you export a conda environment to a file the file may also contain a host-specific <code>prefix</code> line, which should be removed if you or someone else need to run it elsewhere.</p> <p>To use the software installed in the environment remember to activate the environment first using <pre><code>conda activate myproject\n</code></pre></p> <p>List available environments with <pre><code>conda env list\n</code></pre></p>"},{"location":"software/conda/#installing-packages-using-pip-within-conda-environments","title":"Installing packages using pip within conda environments","text":"<p>Software that can only be installed with pip have to be installed in a Conda environment by using pip inside the environment. While issues can arise, per the Conda guide for using pip in a Conda environment, there are some best practices to follow to reduce their likelihood:</p> <ul> <li>Use pip only after conda package installs</li> <li>Use conda environments for isolation (Don't perform pip installs in the <code>base</code> environment)</li> <li>Recreate the entire environment if changes are needed after pip packages have been installed</li> <li>Use <code>--no-cache-dir</code> with any <code>pip install</code> commands</li> </ul> <p>After activating the conda environment an install command would look like the following: <pre><code>$ python3 -m pip install &lt;package&gt; --no-cache-dir\n</code></pre></p> <p>If you then export the conda environment to a YAML file using <code>conda env export &gt; requirements.yml</code>, software dependencies installed using pip should show under a separate <code>- pip:</code> field, for example: <pre><code>name: myproject\nchannels:\n  - bioconda\ndependencies:\n - minimap2=2.26\n - samtools=1.18\n - pip:\n   - virtualenv==20.25.0\n</code></pre></p> <p>Be aware that specific versions are specified using double <code>==</code> with <code>pip</code> dependencies.</p>"},{"location":"software/conda/#r-and-installing-r-packages-within-conda-environments","title":"R and installing R packages within conda environments","text":"<p>Use the notation <code>r-{package}</code> to install R and required R packages within an environment, see the list of packages here. Alternatively using renv is highly recommended for project reproducibility and portability if you need to install many packages.</p>"},{"location":"software/conda/#vs-code-and-conda-environments","title":"VS Code and conda environments","text":"<p>To ensure VS Code uses for example R and Python installations in conda environments, you can make a file <code>.vscode/settings.json</code> in the current project folder and write for example: <pre><code>{\n  \"r.rterm.linux\": \"${userHome}/.conda/envs/myproject/bin/R\",\n  \"python.defaultInterpreterPath\": \"${userHome}/.conda/envs/myproject/bin/python\"\n}\n</code></pre></p> <p>You can also place the <code>settings.json</code> file at <code>$HOME/.config/Code/User/settings.json</code> instead to make the settings apply for all projects. If any <code>.vscode/settings.json</code> files are present in individual project folders, they will take precedence over <code>$HOME/.config/Code/User/settings.json</code>.</p> <p>Read more details here.</p>"},{"location":"software/containers/","title":"Containers","text":"<p>Containers provide a convenient and portable way to package and run applications in a completely isolated and self-contained environment, making it easy to manage dependencies and ensure complete reproducibility and portability. Compared to conda environments or software modules containers are always based on a base operating system image, usually Linux, ensuring that even the operating system is under control. Once a container is built and working as intended, it will run exactly the same forever, whereever, and is therefore the best way to bundle and distribute production-level workflows. By containerizing the application platform and its dependencies, differences in OS distributions and underlying infrastructure are abstracted away completely. Linux containers allow users to:</p> <ul> <li>Use software with complicated dependencies and environment requirements</li> <li>Run an application container from the Sylabs Container Library, Docker Hub, or from self-made images from the GitHub container registry</li> <li>Use a package manager (like apt or yum) to install software without changing anything on the host system or require elevated privileges</li> <li>Run an application that was built for a different distribution of Linux than the host OS</li> <li>Run the latest released software built for newer Linux OS versions than that present on HPC systems</li> <li>Archive an analysis for long-term reproducibility and/or publication</li> </ul>"},{"location":"software/containers/#singularityapptainer","title":"Singularity/Apptainer","text":"<p>Singularity/Apptainer is a tool for running software containers on HPC systems, but is made specifically with scientific computing in mind. Singularity allows running Docker and any other OCI-based container natively and is a replacement for Docker on HPC systems. Singularity has a few extra advantages:</p> <ul> <li>Security: a user in the container is the same user with the same privileges/permissions as the one running the container, so no privilege escalation is possible</li> <li>Ease of deployment: no daemon running as root on each node, a container is simply an executable</li> <li>Ability to run workflows that require MPI and GPU support</li> </ul>"},{"location":"software/containers/#building-container-images","title":"Building container images","text":"<p>Building your own containers using native <code>apptainer build</code> or <code>docker build</code> commands are not possible on BioCloud due to the specific user authentication and security configuration used. Instead you can build containers using either cotainr or enroot. Other alternative ways for building containers are:</p> <ul> <li>Using your own system (laptop/workstation) where you have root/elevated privileges to install Singularity or Docker and build containers, then transfer the container image file(s) to the BioCloud or publish it to a public container registry</li> <li>Use a free cloud container build service like https://cloud.sylabs.io or https://hub.docker.com/</li> <li>Publish a <code>Dockerfile</code> to a GitHub repository and use GitHub actions to build and publish the container to the GitHub container registry</li> </ul>"},{"location":"software/containers/#bundling-conda-environments-in-a-container","title":"Bundling conda environments in a container","text":"<p>cotainr is perhaps the easiest way to bundle conda environments inside an apptainer container by simply giving a path to an environment yaml file (see the conda page) and choosing a base image from for example docker hub:</p> <pre><code>cotainr build --base-image docker://ubuntu:22.04 --conda-env condaenv.yml myproject.sif\n</code></pre> <p>This will produce a single file anyone can use anywhere, regardless of platform and local differences in setup, etc.</p>"},{"location":"software/containers/#pre-built-container-images","title":"Pre-built container images","text":"<p>Usually it's not needed to build a container yourself unless you want to customize things in detail, since there are plenty of pre-built images already available that work straight of the box. For bioinformatic software the community-driven project biocontainers.pro should have anything you need, and if not - you can contribute! If you need a container with multiple tools installed see multi-package containers.</p>"},{"location":"software/containers/#running-a-container","title":"Running a container","text":"<pre><code># pull a container\n$ apptainer pull ubuntu_22.04.sif docker://ubuntu:22.04\n\n# run a container with default options\n$ apptainer run ubuntu_22.04.sif yourcommand --someoption somefile\n\n# start an interactive shell within a container\n$ apptainer shell ubuntu_22.04.sif\n</code></pre> <p>You almost always also need to bind/mount a folder from the host machine to the container, so that it's available inside the container for input/output to the particular tool you need to use. With Singularity/Apptainer the <code>/tmp</code> folder, the current folder, and your home folder are always mounted by default. To mount additional folders use <code>-B</code>, for example: <pre><code># mount with the same path inside the container as on the host\napptainer run -B /databases ubuntu_22.04.sif yourcommand --someoption somefile\n\n# mount at a different path inside the container\napptainer run -B /databases:/some/other/path/databases ubuntu_22.04.sif yourcommand --someoption somefile\n</code></pre></p> <p>For additional guidance see the Apptainer usage guide. If you need to use a GPU with apptainer use the <code>--nvccli</code> flag, not <code>--nv</code>.</p>"},{"location":"software/containers/#docker-containers","title":"Docker containers","text":"<p>Docker itself is not supported directly for non-admin users due to security and compatibility issues with our user authentication mechanism, but you can instead just run them through apptainer by prepending <code>docker://</code> to the container path, see this page.</p>"},{"location":"software/containers/#pyxis-slurm-plugin","title":"pyxis SLURM plugin","text":"<p>The NVIDIA pyxis SLURM spank plugin are also installed and configured, allowing individual SLURM jobs to run inside a container easily by simply using the <code>--container-image</code> option to the <code>srun</code>, <code>sbatch</code>, and <code>salloc</code> commands, see examples here.</p>"},{"location":"software/modules/","title":"Modules (LMOD)","text":"<p>Software modules are a convenient way for administrators to provide pre-installed software for users using lmod and EasyBuild. Software modules are loaded by manipulating the user's environment where the software is made available by replacing default paths to where binaries are found. This makes it possible to isolate software and all its dependencies from the operating system to ensure compatibility on most platforms. Modules are designed and installed using declarative build scripts, most of which are available from Easybuilders.</p> <p>Currently installing new modules is not supported, but modules installed previously are still available through the <code>module</code> command. It's recommended to install software yourself using either conda environments or containers instead.</p>"},{"location":"software/modules/#list-available-modules","title":"List available modules","text":"<pre><code>$ module avail\n</code></pre>"},{"location":"software/modules/#search-modules","title":"Search modules","text":"<pre><code>$ module spider\n</code></pre>"},{"location":"software/modules/#load-a-module","title":"Load a module","text":"<p>When loading modules it's important not to load more than one at a time since conflicts between dependencies of the individual tools may arise. If you need more than one, you need to unload any modules loaded previously using <code>module purge</code>. <pre><code>$ module load minimap2\n</code></pre></p>"},{"location":"software/modules/#unload-a-module","title":"Unload a module","text":"<pre><code>$ module unload minimap2\n</code></pre>"}]}